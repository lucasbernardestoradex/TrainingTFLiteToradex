{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF8ysCfYKgTP"
      },
      "source": [
        "# TensorFlow Lite Object Detection API\n",
        "**Author:** Evan Juras, [EJ Technology Consultants](https://ejtech.io), Lucas Pires Bernardes\n",
        "\n",
        "**Last updated:** 4/12/24\n",
        "\n",
        "**Original GitHub:** [TensorFlow Lite Object Detection](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi)\n",
        "\n",
        "# Introduction\n",
        "\n",
        "This notebook uses [the TensorFlow 2 Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) to train an SSD-MobileNet model or EfficientDet model with a custom dataset and convert it to TensorFlow Lite format. By working through this Jupyter notebook, you'll be able to create and download a TFLite model that you can run on your PC, an Android phone, or an edge device like the Toradex modules.\n",
        "\n",
        "<p align=center>\n",
        "<video width=\"800\" height=\"600\" controls>\n",
        "  <source src=\"vscode_images/VID_20240412_124305.mp4\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "</video>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VAvZo8qE4u5"
      },
      "source": [
        "# 1.&nbsp;Gather and Label Training Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag0qD4XiBDcz"
      },
      "source": [
        "Before we start training, we need to gather and label images that will be used for training the object detection model. A good starting point for a proof-of-concept model is 200 images. The training images should have random objects in the image along with the desired objects, and should have a variety of backgrounds and lighting conditions.\n",
        "\n",
        "Watch the YouTube video below for instructions and tips on how to gather and label images for training an object detection model.\n",
        "\n",
        "<p align=center>\n",
        "<a href=\"https://www.youtube.com/watch?v=v0ssiOY6cfg\" target=\"_blank\"><img src=\"https://s3.us-west-1.amazonaws.com/evanjuras.com/img/thumbnail-capture-label-data.png\" height=\"240\"><br>\n",
        "<i>Watch this video to learn how to capture and label images.</i></a>\n",
        "</p>\n",
        "\n",
        "When you've finished gathering and labeling images, you should have a folder full of images and corresponding .xml data annotation file for each image. An example of a labeled image and the image folder for Toradex detector model are shown below.\n",
        "\n",
        "![](vscode_images/labels_vscode.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.&nbsp;Python version\n",
        "\n",
        "**Warning**: This code has been tested on different versions of Python and has been verified to only works below **Python 3.9**. This was tested on **Python 3.9**.\n",
        "\n",
        "It is recommended to use the **Conda** software to install a different version of Python and install the package **ipykernel** inside the virtual environmental to run a Jupyter notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxb8_h-QFErO"
      },
      "source": [
        "# 3.&nbsp;Install TensorFlow Object Detection Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7EOtpvlLeS0"
      },
      "source": [
        "First, we'll install the TensorFlow Object Detection API in this Google Colab instance. This requires cloning the [TensorFlow models repository](https://github.com/tensorflow/models) and running a couple installation commands. Click the play button to run the following sections of code.\n",
        "\n",
        "The latest version of TensorFlow this Colab has been verified to work with is TF v2.8.0.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ypWGYdPlLRUN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping Cython as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCloning into 'models'...\n",
            "remote: Enumerating objects: 4088, done.\u001b[K\n",
            "remote: Counting objects: 100% (4088/4088), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3080/3080), done.\u001b[K\n",
            "remote: Total 4088 (delta 1185), reused 2000 (delta 947), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (4088/4088), 44.63 MiB | 16.74 MiB/s, done.\n",
            "Resolving deltas: 100% (1185/1185), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone the tensorflow models repository from GitHub\n",
        "!pip uninstall Cython -y # Temporary fix for \"No module named 'object_detection'\" error\n",
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6QPmVBSlLTzM"
      },
      "outputs": [],
      "source": [
        "# Copy setup files into models/research folder\n",
        "!cd models/research/ \\\n",
        "&& protoc object_detection/protos/*.proto --python_out=.\n",
        "#cp object_detection/packages/tf2/setup.py ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NRBnuCKjM4Bd"
      },
      "outputs": [],
      "source": [
        "# Modify setup.py file to install the tf-models-official repository targeted at TF v2.8.0\n",
        "import re\n",
        "with open('models/research/object_detection/packages/tf2/setup.py') as f:\n",
        "    s = f.read()\n",
        "\n",
        "with open('models/research/setup.py', 'w') as f:\n",
        "    # Set fine_tune_checkpoint path\n",
        "    s = re.sub('tf-models-official>=2.5.1',\n",
        "               'tf-models-official==2.8.0', s)\n",
        "    f.write(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When your password as requested, remember to use \"\\\\\" before special characters, e.g. \"Romero&Brito\" should be written as \"Romero\\\\&Brito\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[sudo] password for lucas_bernardes: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Get:1 file:/var/cuda-repo-ubuntu1804-11-0-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-ubuntu1804-11-0-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-ubuntu1804-11-0-local  Release [564 B]\n",
            "Get:2 file:/var/cuda-repo-ubuntu1804-11-0-local  Release [564 B]\n",
            "Hit:5 http://deb.debian.org/debian bookworm InRelease\n",
            "Get:6 http://deb.debian.org/debian bookworm-updates InRelease [55.4 kB]\n",
            "Hit:7 https://packages.microsoft.com/debian/12/prod bookworm InRelease\n",
            "Hit:8 https://packages.microsoft.com/repos/microsoft-debian-bookworm-prod bookworm InRelease\n",
            "Get:4 https://packages.microsoft.com/repos/code stable InRelease [3,590 B]\n",
            "Hit:9 https://download.docker.com/linux/debian bookworm InRelease\n",
            "Get:10 https://packages.microsoft.com/repos/code stable/main amd64 Packages [16.8 kB]\n",
            "Get:11 https://packages.microsoft.com/repos/code stable/main armhf Packages [16.9 kB]\n",
            "Get:12 https://packages.microsoft.com/repos/code stable/main arm64 Packages [17.0 kB]\n",
            "Hit:13 https://dl.google.com/linux/chrome/deb stable InRelease\n",
            "Hit:14 https://packages.cloud.google.com/apt coral-edgetpu-stable InRelease\n",
            "Fetched 110 kB in 6s (18.5 kB/s)\n",
            "Reading package lists...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W: file:/var/cuda-repo-ubuntu1804-11-0-local/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: https://packages.cloud.google.com/apt/dists/coral-edgetpu-stable/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "password = getpass.getpass()\n",
        "command = \"sudo -S apt-get update\" #can be any command but don't forget -S as it enables input from stdin\n",
        "os.system('echo %s | %s' % (password, command))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OLDnCkLLwLr6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyyaml==5.3\n",
            "  Using cached PyYAML-5.3-cp39-cp39-linux_x86_64.whl\n",
            "Installing collected packages: pyyaml\n",
            "Successfully installed pyyaml-5.3\n",
            "Processing ./models/research\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting avro-python3 (from object-detection==0.1)\n",
            "  Using cached avro_python3-1.10.2-py3-none-any.whl\n",
            "Collecting apache-beam (from object-detection==0.1)\n",
            "  Using cached apache_beam-2.55.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting pillow (from object-detection==0.1)\n",
            "  Downloading pillow-10.3.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting lxml (from object-detection==0.1)\n",
            "  Downloading lxml-5.2.1-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting matplotlib (from object-detection==0.1)\n",
            "  Downloading matplotlib-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting Cython (from object-detection==0.1)\n",
            "  Using cached Cython-3.0.10-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting contextlib2 (from object-detection==0.1)\n",
            "  Using cached contextlib2-21.6.0-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting tf-slim (from object-detection==0.1)\n",
            "  Using cached tf_slim-1.1.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: six in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from object-detection==0.1) (1.16.0)\n",
            "Collecting pycocotools (from object-detection==0.1)\n",
            "  Using cached pycocotools-2.0.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting lvis (from object-detection==0.1)\n",
            "  Using cached lvis-0.5.3-py3-none-any.whl.metadata (856 bytes)\n",
            "Collecting scipy (from object-detection==0.1)\n",
            "  Downloading scipy-1.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas (from object-detection==0.1)\n",
            "  Downloading pandas-2.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting tf-models-official==2.8.0 (from object-detection==0.1)\n",
            "  Using cached tf_models_official-2.8.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting tensorflow_io (from object-detection==0.1)\n",
            "  Using cached tensorflow_io-0.36.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting keras (from object-detection==0.1)\n",
            "  Using cached keras-3.2.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting pyparsing==2.4.7 (from object-detection==0.1)\n",
            "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting sacrebleu<=2.2.0 (from object-detection==0.1)\n",
            "  Using cached sacrebleu-2.2.0-py3-none-any.whl.metadata (55 kB)\n",
            "Collecting gin-config (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached gin_config-0.5.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting google-api-python-client>=1.6.7 (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading google_api_python_client-2.125.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting kaggle>=1.3.9 (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading kaggle-1.6.12.tar.gz (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting numpy>=1.15.4 (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting oauth2client (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting opencv-python-headless (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached opencv_python_headless-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tf-models-official==2.8.0->object-detection==0.1) (5.9.8)\n",
            "Collecting py-cpuinfo>=3.3.0 (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
            "Requirement already satisfied: pyyaml<6.0,>=5.1 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tf-models-official==2.8.0->object-detection==0.1) (5.3)\n",
            "Collecting sentencepiece (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached sentencepiece-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting seqeval (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached seqeval-1.2.2-py3-none-any.whl\n",
            "Collecting tensorflow-addons (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached tensorflow_addons-0.23.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-datasets (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached tensorflow_datasets-4.9.3-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting tensorflow-hub>=0.6.0 (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Collecting tensorflow-text~=2.8.0 (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached tensorflow_text-2.8.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting tensorflow~=2.8.0 (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached tensorflow-2.8.4-cp39-cp39-manylinux2010_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from pandas->object-detection==0.1) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas->object-detection==0.1)\n",
            "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->object-detection==0.1)\n",
            "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting portalocker (from sacrebleu<=2.2.0->object-detection==0.1)\n",
            "  Using cached portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting regex (from sacrebleu<=2.2.0->object-detection==0.1)\n",
            "  Using cached regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting tabulate>=0.8.9 (from sacrebleu<=2.2.0->object-detection==0.1)\n",
            "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting colorama (from sacrebleu<=2.2.0->object-detection==0.1)\n",
            "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting absl-py>=0.2.2 (from tf-slim->object-detection==0.1)\n",
            "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting crcmod<2.0,>=1.7 (from apache-beam->object-detection==0.1)\n",
            "  Using cached crcmod-1.7-cp39-cp39-linux_x86_64.whl\n",
            "Collecting orjson<4,>=3.9.7 (from apache-beam->object-detection==0.1)\n",
            "  Using cached orjson-3.10.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
            "Collecting dill<0.3.2,>=0.3.1.1 (from apache-beam->object-detection==0.1)\n",
            "  Using cached dill-0.3.1.1-py3-none-any.whl\n",
            "Collecting cloudpickle~=2.2.1 (from apache-beam->object-detection==0.1)\n",
            "  Using cached cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting fastavro<2,>=0.23.6 (from apache-beam->object-detection==0.1)\n",
            "  Using cached fastavro-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting fasteners<1.0,>=0.3 (from apache-beam->object-detection==0.1)\n",
            "  Using cached fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting grpcio!=1.48.0,<2,>=1.33.1 (from apache-beam->object-detection==0.1)\n",
            "  Using cached grpcio-1.62.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam->object-detection==0.1)\n",
            "  Using cached hdfs-2.7.3-py3-none-any.whl\n",
            "Collecting httplib2<0.23.0,>=0.8 (from apache-beam->object-detection==0.1)\n",
            "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting js2py<1,>=0.74 (from apache-beam->object-detection==0.1)\n",
            "  Using cached Js2Py-0.74-py3-none-any.whl.metadata (868 bytes)\n",
            "Collecting jsonschema<5.0.0,>=4.0.0 (from apache-beam->object-detection==0.1)\n",
            "  Using cached jsonschema-4.21.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting jsonpickle<4.0.0,>=3.0.0 (from apache-beam->object-detection==0.1)\n",
            "  Using cached jsonpickle-3.0.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting objsize<0.8.0,>=0.6.1 (from apache-beam->object-detection==0.1)\n",
            "  Using cached objsize-0.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from apache-beam->object-detection==0.1) (24.0)\n",
            "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam->object-detection==0.1)\n",
            "  Using cached pymongo-4.6.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting proto-plus<2,>=1.7.1 (from apache-beam->object-detection==0.1)\n",
            "  Using cached proto_plus-1.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 (from apache-beam->object-detection==0.1)\n",
            "  Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting pydot<2,>=1.2.0 (from apache-beam->object-detection==0.1)\n",
            "  Using cached pydot-1.4.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting requests<3.0.0,>=2.24.0 (from apache-beam->object-detection==0.1)\n",
            "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from apache-beam->object-detection==0.1) (4.11.0)\n",
            "Collecting zstandard<1,>=0.18.0 (from apache-beam->object-detection==0.1)\n",
            "  Using cached zstandard-0.22.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
            "Collecting pyarrow<15.0.0,>=3.0.0 (from apache-beam->object-detection==0.1)\n",
            "  Using cached pyarrow-14.0.2-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting pyarrow-hotfix<1 (from apache-beam->object-detection==0.1)\n",
            "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting rich (from keras->object-detection==0.1)\n",
            "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting namex (from keras->object-detection==0.1)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
            "Collecting h5py (from keras->object-detection==0.1)\n",
            "  Downloading h5py-3.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting optree (from keras->object-detection==0.1)\n",
            "  Downloading optree-0.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ml-dtypes (from keras->object-detection==0.1)\n",
            "  Downloading ml_dtypes-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting cycler>=0.10.0 (from lvis->object-detection==0.1)\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting kiwisolver>=1.1.0 (from lvis->object-detection==0.1)\n",
            "  Using cached kiwisolver-1.4.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting opencv-python>=4.1.0.25 (from lvis->object-detection==0.1)\n",
            "  Using cached opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib->object-detection==0.1)\n",
            "  Downloading contourpy-1.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib->object-detection==0.1)\n",
            "  Downloading fonttools-4.51.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.5/159.5 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-resources>=3.2.0 (from matplotlib->object-detection==0.1)\n",
            "  Using cached importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem==0.36.0 (from tensorflow_io->object-detection==0.1)\n",
            "  Using cached tensorflow_io_gcs_filesystem-0.36.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 (from google-api-python-client>=1.6.7->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached google_auth-2.29.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client>=1.6.7->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client>=1.6.7->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached google_api_core-2.18.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client>=1.6.7->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1)\n",
            "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: zipp>=3.1.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->object-detection==0.1) (3.18.1)\n",
            "Collecting tzlocal>=1.2 (from js2py<1,>=0.74->apache-beam->object-detection==0.1)\n",
            "  Using cached tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting pyjsparser>=2.5.1 (from js2py<1,>=0.74->apache-beam->object-detection==0.1)\n",
            "  Using cached pyjsparser-2.7.1-py3-none-any.whl\n",
            "Collecting attrs>=22.2.0 (from jsonschema<5.0.0,>=4.0.0->apache-beam->object-detection==0.1)\n",
            "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.0.0->apache-beam->object-detection==0.1)\n",
            "  Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema<5.0.0,>=4.0.0->apache-beam->object-detection==0.1)\n",
            "  Using cached referencing-0.34.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting rpds-py>=0.7.1 (from jsonschema<5.0.0,>=4.0.0->apache-beam->object-detection==0.1)\n",
            "  Using cached rpds_py-0.18.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting certifi>=2023.7.22 (from kaggle>=1.3.9->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting tqdm (from kaggle>=1.3.9->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting python-slugify (from kaggle>=1.3.9->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting urllib3 (from kaggle>=1.3.9->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting bleach (from kaggle>=1.3.9->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached bleach-6.1.0-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam->object-detection==0.1)\n",
            "  Using cached dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1)\n",
            "  Using cached charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=1.12 (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Collecting gast>=0.2.1 (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting libclang>=9.0.1 (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow~=2.8.0 (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached tensorflow-2.8.3-cp39-cp39-manylinux2010_x86_64.whl.metadata (2.9 kB)\n",
            "  Using cached tensorflow-2.8.2-cp39-cp39-manylinux2010_x86_64.whl.metadata (2.9 kB)\n",
            "  Using cached tensorflow-2.8.1-cp39-cp39-manylinux2010_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: setuptools in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (68.2.2)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting wrapt>=1.11.0 (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached wrapt-1.16.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting tensorboard<2.9,>=2.8 (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached tensorboard-2.8.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tensorflow-estimator<2.9,>=2.8 (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached tensorflow_estimator-2.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras (from object-detection==0.1)\n",
            "  Using cached keras-2.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting tf-keras>=2.14.1 (from tensorflow-hub>=0.6.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting absl-py>=0.2.2 (from tf-slim->object-detection==0.1)\n",
            "  Using cached absl_py-1.4.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting dm-tree~=0.1.1 (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached dm_tree-0.1.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting pyasn1>=0.1.7 (from oauth2client->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting pyasn1-modules>=0.0.5 (from oauth2client->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting rsa>=3.1.4 (from oauth2client->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting scikit-learn>=0.21.3 (from seqeval->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading scikit_learn-1.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting array-record (from tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading array_record-0.5.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (699 bytes)\n",
            "Collecting click (from tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting etils>=0.9.0 (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached etils-1.5.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting promise (from tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached promise-2.3-py3-none-any.whl\n",
            "Collecting tensorflow-metadata (from tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached tensorflow_metadata-1.14.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting toml (from tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (0.41.2)\n",
            "Collecting fsspec (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached googleapis_common_protos-1.63.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading joblib-1.4.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Collecting werkzeug>=0.11.15 (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading werkzeug-3.0.2-py3-none-any.whl.metadata (4.1 kB)\n",
            "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf-keras>=2.14.1 (from tensorflow-hub>=0.6.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading tf_keras-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Using cached tf_keras-2.15.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting webencodings (from bleach->kaggle>=1.3.9->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting text-unidecode>=1.3 (from python-slugify->kaggle>=1.3.9->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 (from apache-beam->object-detection==0.1)\n",
            "  Using cached protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (679 bytes)\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (7.1.0)\n",
            "Collecting MarkupSafe>=2.1.1 (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached MarkupSafe-2.1.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich->keras->object-detection==0.1)\n",
            "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from rich->keras->object-detection==0.1) (2.17.2)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras->object-detection==0.1)\n",
            "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "Using cached tf_models_official-2.8.0-py2.py3-none-any.whl (2.2 MB)\n",
            "Downloading pandas-2.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
            "Downloading scipy-1.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "Using cached apache_beam-2.55.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.1 MB)\n",
            "Using cached contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
            "Using cached Cython-3.0.10-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "Using cached lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Downloading matplotlib-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.3.0-cp39-cp39-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading lxml-5.2.1-cp39-cp39-manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached pycocotools-2.0.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
            "Using cached tensorflow_io-0.36.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.5 MB)\n",
            "Using cached tensorflow_io_gcs_filesystem-0.36.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
            "Downloading contourpy-1.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (304 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.0/305.0 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Using cached fastavro-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "Using cached fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Downloading fonttools-4.51.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading google_api_python_client-2.125.0-py2.py3-none-any.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached grpcio-1.62.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
            "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
            "Using cached importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
            "Using cached Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
            "Using cached jsonpickle-3.0.4-py3-none-any.whl (39 kB)\n",
            "Using cached jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
            "Using cached kiwisolver-1.4.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "Using cached numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Using cached objsize-0.7.0-py3-none-any.whl (11 kB)\n",
            "Using cached opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.2 MB)\n",
            "Using cached orjson-3.10.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "Using cached proto_plus-1.23.0-py3-none-any.whl (48 kB)\n",
            "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Using cached pyarrow-14.0.2-cp39-cp39-manylinux_2_28_x86_64.whl (38.0 MB)\n",
            "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Using cached pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
            "Using cached pymongo-4.6.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (676 kB)\n",
            "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "Using cached regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
            "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Using cached tensorflow-2.8.1-cp39-cp39-manylinux2010_x86_64.whl (498.0 MB)\n",
            "Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
            "Using cached tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "Using cached tensorflow_text-2.8.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
            "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "Using cached zstandard-0.22.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Using cached gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
            "Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
            "Using cached opencv_python_headless-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\n",
            "Using cached portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Using cached sentencepiece-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Using cached tensorflow_addons-0.23.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "Downloading tensorflow_datasets-4.9.3-py3-none-any.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
            "Using cached charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "Using cached dm_tree-0.1.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (153 kB)\n",
            "Using cached dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "Using cached etils-1.5.2-py3-none-any.whl (140 kB)\n",
            "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
            "Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
            "Using cached google_api_core-2.18.0-py3-none-any.whl (138 kB)\n",
            "Using cached google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
            "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
            "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Downloading h5py-3.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
            "Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached referencing-0.34.0-py3-none-any.whl (26 kB)\n",
            "Using cached rpds_py-0.18.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Downloading scikit_learn-1.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "Using cached tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\n",
            "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading tf_keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Using cached tzlocal-5.2-py3-none-any.whl (17 kB)\n",
            "Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
            "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
            "Using cached wrapt-1.16.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
            "Downloading array_record-0.5.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached bleach-6.1.0-py3-none-any.whl (162 kB)\n",
            "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
            "Using cached python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
            "Using cached tensorflow_metadata-1.14.0-py3-none-any.whl (28 kB)\n",
            "Using cached protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
            "Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
            "Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Using cached googleapis_common_protos-1.63.0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading joblib-1.4.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.2/301.2 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
            "Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
            "Using cached threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading werkzeug-3.0.2-py3-none-any.whl (226 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
            "Using cached MarkupSafe-2.1.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "Building wheels for collected packages: object-detection, kaggle\n",
            "  Building wheel for object-detection (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1655472 sha256=b052971359bab7d10a48e2a00de804a3604160cd0c817fff8fd3b12391caaee8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ya31bae7/wheels/b9/2d/4a/eafcbccfcfcedd9bfa773a64d7c06a0198a34d00aa08b82141\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.6.12-py3-none-any.whl size=102971 sha256=930b18d376a5bd4b8cd297330cb45ff53df6a7cab2cc8b265af793c9621a3499\n",
            "  Stored in directory: /home/lucas_bernardes/.cache/pip/wheels/00/74/7d/52c235c074504ceced3dee430f10873c95d8a9e81263ece28e\n",
            "Successfully built object-detection kaggle\n",
            "Installing collected packages: webencodings, text-unidecode, tensorflow-estimator, tensorboard-plugin-wit, sentencepiece, pytz, pyjsparser, py-cpuinfo, libclang, keras, gin-config, flatbuffers, docopt, dm-tree, crcmod, zstandard, wrapt, urllib3, uritemplate, tzlocal, tzdata, typeguard, tqdm, toml, threadpoolctl, tf-keras, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, tabulate, rpds-py, regex, python-slugify, pyparsing, pyasn1, pyarrow-hotfix, protobuf, promise, portalocker, pillow, orjson, objsize, oauthlib, numpy, MarkupSafe, lxml, kiwisolver, jsonpickle, joblib, importlib-resources, idna, grpcio, google-pasta, gast, fsspec, fonttools, fasteners, fastavro, etils, dnspython, dill, Cython, cycler, contextlib2, colorama, cloudpickle, click, charset-normalizer, certifi, cachetools, bleach, avro-python3, attrs, astunparse, absl-py, werkzeug, tf-slim, tensorflow-model-optimization, tensorflow_io, tensorflow-hub, tensorflow-addons, scipy, sacrebleu, rsa, requests, referencing, pymongo, pydot, pyasn1-modules, pyarrow, proto-plus, pandas, opt-einsum, opencv-python-headless, opencv-python, markdown, keras-preprocessing, js2py, httplib2, h5py, googleapis-common-protos, contourpy, tensorflow-metadata, scikit-learn, requests-oauthlib, oauth2client, matplotlib, kaggle, jsonschema-specifications, hdfs, google-auth, seqeval, pycocotools, lvis, jsonschema, google-auth-oauthlib, google-auth-httplib2, google-api-core, array-record, tensorboard, google-api-python-client, apache-beam, tensorflow-datasets, tensorflow, tensorflow-text, tf-models-official, object-detection\n",
            "Successfully installed Cython-3.0.10 MarkupSafe-2.1.5 absl-py-1.4.0 apache-beam-2.55.1 array-record-0.5.1 astunparse-1.6.3 attrs-23.2.0 avro-python3-1.10.2 bleach-6.1.0 cachetools-5.3.3 certifi-2024.2.2 charset-normalizer-3.3.2 click-8.1.7 cloudpickle-2.2.1 colorama-0.4.6 contextlib2-21.6.0 contourpy-1.2.1 crcmod-1.7 cycler-0.12.1 dill-0.3.1.1 dm-tree-0.1.8 dnspython-2.6.1 docopt-0.6.2 etils-1.5.2 fastavro-1.9.4 fasteners-0.19 flatbuffers-24.3.25 fonttools-4.51.0 fsspec-2024.3.1 gast-0.5.4 gin-config-0.5.0 google-api-core-2.18.0 google-api-python-client-2.125.0 google-auth-2.29.0 google-auth-httplib2-0.2.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 googleapis-common-protos-1.63.0 grpcio-1.62.1 h5py-3.11.0 hdfs-2.7.3 httplib2-0.22.0 idna-3.7 importlib-resources-6.4.0 joblib-1.4.0 js2py-0.74 jsonpickle-3.0.4 jsonschema-4.21.1 jsonschema-specifications-2023.12.1 kaggle-1.6.12 keras-2.8.0 keras-preprocessing-1.1.2 kiwisolver-1.4.5 libclang-18.1.1 lvis-0.5.3 lxml-5.2.1 markdown-3.6 matplotlib-3.8.4 numpy-1.26.4 oauth2client-4.1.3 oauthlib-3.2.2 object-detection-0.1 objsize-0.7.0 opencv-python-4.9.0.80 opencv-python-headless-4.9.0.80 opt-einsum-3.3.0 orjson-3.10.0 pandas-2.2.2 pillow-10.3.0 portalocker-2.8.2 promise-2.3 proto-plus-1.23.0 protobuf-3.20.3 py-cpuinfo-9.0.0 pyarrow-14.0.2 pyarrow-hotfix-0.6 pyasn1-0.6.0 pyasn1-modules-0.4.0 pycocotools-2.0.7 pydot-1.4.2 pyjsparser-2.7.1 pymongo-4.6.3 pyparsing-2.4.7 python-slugify-8.0.4 pytz-2024.1 referencing-0.34.0 regex-2023.12.25 requests-2.31.0 requests-oauthlib-2.0.0 rpds-py-0.18.0 rsa-4.9 sacrebleu-2.2.0 scikit-learn-1.4.2 scipy-1.13.0 sentencepiece-0.2.0 seqeval-1.2.2 tabulate-0.9.0 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.1 tensorflow-addons-0.23.0 tensorflow-datasets-4.9.3 tensorflow-estimator-2.8.0 tensorflow-hub-0.16.1 tensorflow-io-gcs-filesystem-0.36.0 tensorflow-metadata-1.14.0 tensorflow-model-optimization-0.8.0 tensorflow-text-2.8.2 tensorflow_io-0.36.0 termcolor-2.4.0 text-unidecode-1.3 tf-keras-2.15.0 tf-models-official-2.8.0 tf-slim-1.1.0 threadpoolctl-3.4.0 toml-0.10.2 tqdm-4.66.2 typeguard-2.13.3 tzdata-2024.1 tzlocal-5.2 uritemplate-4.1.1 urllib3-2.2.1 webencodings-0.5.1 werkzeug-3.0.2 wrapt-1.16.0 zstandard-0.22.0\n",
            "Collecting tensorflow==2.8.0\n",
            "  Using cached tensorflow-2.8.0-cp39-cp39-manylinux2010_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorflow==2.8.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorflow==2.8.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorflow==2.8.0) (24.3.25)\n",
            "Requirement already satisfied: gast>=0.2.1 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorflow==2.8.0) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorflow==2.8.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorflow==2.8.0) (3.11.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorflow==2.8.0) (1.1.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorflow==2.8.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorflow==2.8.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorflow==2.8.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorflow==2.8.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorflow==2.8.0) (68.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorflow==2.8.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorflow==2.8.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorflow==2.8.0) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorflow==2.8.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorflow==2.8.0) (2.8.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8.0)\n",
            "  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorflow==2.8.0) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorflow==2.8.0) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorflow==2.8.0) (1.62.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.29.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (7.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.1.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.18.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n",
            "Using cached tensorflow-2.8.0-cp39-cp39-manylinux2010_x86_64.whl (497.6 MB)\n",
            "Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "Installing collected packages: tf-estimator-nightly, tensorflow\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.1\n",
            "    Uninstalling tensorflow-2.8.1:\n",
            "      Successfully uninstalled tensorflow-2.8.1\n",
            "Successfully installed tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n",
            "Collecting tensorflow_io==0.23.1\n",
            "  Using cached tensorflow_io-0.23.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem==0.23.1 (from tensorflow_io==0.23.1)\n",
            "  Using cached tensorflow_io_gcs_filesystem-0.23.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\n",
            "Using cached tensorflow_io-0.23.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.1 MB)\n",
            "Using cached tensorflow_io_gcs_filesystem-0.23.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
            "Installing collected packages: tensorflow-io-gcs-filesystem, tensorflow_io\n",
            "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
            "    Found existing installation: tensorflow-io-gcs-filesystem 0.36.0\n",
            "    Uninstalling tensorflow-io-gcs-filesystem-0.36.0:\n",
            "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.36.0\n",
            "  Attempting uninstall: tensorflow_io\n",
            "    Found existing installation: tensorflow-io 0.36.0\n",
            "    Uninstalling tensorflow-io-0.36.0:\n",
            "      Successfully uninstalled tensorflow-io-0.36.0\n",
            "Successfully installed tensorflow-io-gcs-filesystem-0.23.1 tensorflow_io-0.23.1\n",
            "--2024-04-15 13:07:57--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.52.179\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.52.179|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 190 [application/octet-stream]\n",
            "Saving to: ‘cuda-ubuntu1804.pin’\n",
            "\n",
            "cuda-ubuntu1804.pin 100%[===================>]     190  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-15 13:07:57 (32.6 MB/s) - ‘cuda-ubuntu1804.pin’ saved [190/190]\n",
            "\n",
            "--2024-04-15 13:07:57--  http://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.52.179\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.52.179|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb [following]\n",
            "--2024-04-15 13:07:57--  https://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.52.179|:443... connected.\n",
            "HTTP request sent, awaiting response... "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[sudo] password for lucas_bernardes: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200 OK\n",
            "Length: 2273753684 (2.1G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb’\n",
            "\n",
            "cuda-repo-ubuntu180 100%[===================>]   2.12G  17.7MB/s    in 2m 4s   \n",
            "\n",
            "2024-04-15 13:10:01 (17.5 MB/s) - ‘cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb’ saved [2273753684/2273753684]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[sudo] password for lucas_bernardes: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Reading database ... 240149 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb ...\n",
            "Unpacking cuda-repo-ubuntu1804-11-0-local (11.0.2-450.51.05-1) over (11.0.2-450.51.05-1) ...\n",
            "Setting up cuda-repo-ubuntu1804-11-0-local (11.0.2-450.51.05-1) ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[sudo] password for lucas_bernardes: Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK\n",
            "Reading package lists...\n",
            "Building dependency tree..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[sudo] password for lucas_bernardes: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Reading state information...\n",
            "The following package was automatically installed and is no longer required:\n",
            "  linux-image-6.1.0-10-amd64\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  ca-certificates-java cuda-command-line-tools-11-0 cuda-compiler-11-0\n",
            "  cuda-cudart-11-0 cuda-cudart-dev-11-0 cuda-cuobjdump-11-0 cuda-cupti-11-0\n",
            "  cuda-cupti-dev-11-0 cuda-documentation-11-0 cuda-driver-dev-11-0\n",
            "  cuda-gdb-11-0 cuda-libraries-11-0 cuda-libraries-dev-11-0 cuda-memcheck-11-0\n",
            "  cuda-nsight-11-0 cuda-nsight-compute-11-0 cuda-nsight-systems-11-0\n",
            "  cuda-nvcc-11-0 cuda-nvdisasm-11-0 cuda-nvml-dev-11-0 cuda-nvprof-11-0\n",
            "  cuda-nvprune-11-0 cuda-nvrtc-11-0 cuda-nvrtc-dev-11-0 cuda-nvtx-11-0\n",
            "  cuda-nvvp-11-0 cuda-samples-11-0 cuda-sanitizer-11-0 cuda-tools-11-0\n",
            "  cuda-visual-tools-11-0 default-jre default-jre-headless freeglut3-dev\n",
            "  java-common libcublas-11-0 libcublas-dev-11-0 libcufft-11-0\n",
            "  libcufft-dev-11-0 libcurand-11-0 libcurand-dev-11-0 libcusolver-11-0\n",
            "  libcusolver-dev-11-0 libcusparse-11-0 libcusparse-dev-11-0 libgl1-mesa-dev\n",
            "  libglu1-mesa-dev libglut-dev libglut3.12 libice-dev libnpp-11-0\n",
            "  libnpp-dev-11-0 libnvjpeg-11-0 libnvjpeg-dev-11-0 libsm-dev libxext-dev\n",
            "  libxfixes-dev libxi-dev libxmu-dev libxmu-headers libxt-dev openjdk-17-jre\n",
            "  openjdk-17-jre-headless\n",
            "Suggested packages:\n",
            "  libice-doc libsm-doc libxext-doc libxt-doc fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei | fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  ca-certificates-java cuda-command-line-tools-11-0 cuda-compiler-11-0\n",
            "  cuda-cudart-11-0 cuda-cudart-dev-11-0 cuda-cuobjdump-11-0 cuda-cupti-11-0\n",
            "  cuda-cupti-dev-11-0 cuda-documentation-11-0 cuda-driver-dev-11-0\n",
            "  cuda-gdb-11-0 cuda-libraries-11-0 cuda-libraries-dev-11-0 cuda-memcheck-11-0\n",
            "  cuda-nsight-11-0 cuda-nsight-compute-11-0 cuda-nsight-systems-11-0\n",
            "  cuda-nvcc-11-0 cuda-nvdisasm-11-0 cuda-nvml-dev-11-0 cuda-nvprof-11-0\n",
            "  cuda-nvprune-11-0 cuda-nvrtc-11-0 cuda-nvrtc-dev-11-0 cuda-nvtx-11-0\n",
            "  cuda-nvvp-11-0 cuda-samples-11-0 cuda-sanitizer-11-0 cuda-toolkit-11-0\n",
            "  cuda-tools-11-0 cuda-visual-tools-11-0 default-jre default-jre-headless\n",
            "  freeglut3-dev java-common libcublas-11-0 libcublas-dev-11-0 libcufft-11-0\n",
            "  libcufft-dev-11-0 libcurand-11-0 libcurand-dev-11-0 libcusolver-11-0\n",
            "  libcusolver-dev-11-0 libcusparse-11-0 libcusparse-dev-11-0 libgl1-mesa-dev\n",
            "  libglu1-mesa-dev libglut-dev libglut3.12 libice-dev libnpp-11-0\n",
            "  libnpp-dev-11-0 libnvjpeg-11-0 libnvjpeg-dev-11-0 libsm-dev libxext-dev\n",
            "  libxfixes-dev libxi-dev libxmu-dev libxmu-headers libxt-dev openjdk-17-jre\n",
            "  openjdk-17-jre-headless\n",
            "0 upgraded, 63 newly installed, 0 to remove and 0 not upgraded.\n",
            "Need to get 45.5 MB/1,627 MB of archives.\n",
            "After this operation, 4,200 MB of additional disk space will be used.\n",
            "Do you want to continue? [Y/n] Abort.\n"
          ]
        }
      ],
      "source": [
        "# Install the Object Detection API (NOTE: This block takes about 10 minutes to finish executing)\n",
        "\n",
        "# Need to do a temporary fix with PyYAML because Colab isn't able to install PyYAML v5.4.1\n",
        "!pip install pyyaml==5.3\n",
        "!pip install models/research/\n",
        "\n",
        "# Need to downgrade to TF v2.8.0 due to Colab compatibility bug with TF v2.10 (as of 10/03/22)\n",
        "!pip install tensorflow==2.8.0\n",
        "\n",
        "# Install CUDA version 11.0 (to maintain compatibility with TF v2.8.0)\n",
        "!pip install tensorflow_io==0.23.1\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n",
        "command = \"sudo -S mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\" #can be any command but don't forget -S as it enables input from stdin\n",
        "os.system('echo %s | %s' % (password, command))\n",
        "!wget http://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb\n",
        "command = \"sudo -S dpkg -i cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb\" #can be any command but don't forget -S as it enables input from stdin\n",
        "os.system('echo %s | %s' % (password, command))\n",
        "command = \"sudo -S apt-key add /var/cuda-repo-ubuntu1804-11-0-local/7fa2af80.pub\" #can be any command but don't forget -S as it enables input from stdin\n",
        "os.system('echo %s | %s' % (password, command))\n",
        "command = \"sudo -S apt-get install cuda-toolkit-11-0\" #can be any command but don't forget -S as it enables input from stdin\n",
        "os.system('echo %s | %s' % (password, command))\n",
        "!export LD_LIBRARY_PATH=/usr/local/cuda-11.0/lib64:$LD_LIBRARY_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V7TrfUos-9E"
      },
      "source": [
        "You may get warnings or errors related to package dependencies in the previous code block, but you can ignore them for now.\n",
        "\n",
        "Let's test our installation by running `model_builder_tf2_test.py` to make sure everything is working as expected. Run the following code block and confirm that it finishes without errors. If you get errors, try Googling them or checking the FAQ at the end of this Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-04-15 13:10:17.361506: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2024-04-15 13:10:17.361562: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (lucasb-nb): /proc/driver/nvidia/version does not exist\n",
            "Running tests under Python 3.9.19: /home/lucas_bernardes/miniconda3/envs/test_presentation/bin/python\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
            "2024-04-15 13:10:17.370704: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages/object_detection/builders/model_builder.py:1112: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
            "  logging.warn(('Building experimental DeepMAC meta-arch.'\n",
            "W0415 13:10:17.564641 139975360443648 model_builder.py:1112] Building experimental DeepMAC meta-arch. Some features may be omitted.\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 0.31s\n",
            "I0415 13:10:17.671092 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 0.31s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.23s\n",
            "I0415 13:10:17.899324 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.23s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.19s\n",
            "I0415 13:10:18.086873 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.19s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.1s\n",
            "I0415 13:10:18.185868 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.1s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 0.85s\n",
            "I0415 13:10:19.040204 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 0.85s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "I0415 13:10:19.040949 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.01s\n",
            "I0415 13:10:19.052804 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.01s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.01s\n",
            "I0415 13:10:19.060165 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.01s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.01s\n",
            "I0415 13:10:19.067950 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.01s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.05s\n",
            "I0415 13:10:19.115544 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.05s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.04s\n",
            "I0415 13:10:19.157198 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.04s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.04s\n",
            "I0415 13:10:19.201590 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.04s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.04s\n",
            "I0415 13:10:19.246140 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.04s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.04s\n",
            "I0415 13:10:19.287916 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.04s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.01s\n",
            "I0415 13:10:19.300318 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.01s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "I0415 13:10:19.473198 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
            "I0415 13:10:19.473285 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 64\n",
            "I0415 13:10:19.473309 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 3\n",
            "I0415 13:10:19.474450 139975360443648 efficientnet_model.py:144] round_filter input=32 output=32\n",
            "I0415 13:10:19.482259 139975360443648 efficientnet_model.py:144] round_filter input=32 output=32\n",
            "I0415 13:10:19.482350 139975360443648 efficientnet_model.py:144] round_filter input=16 output=16\n",
            "I0415 13:10:19.508092 139975360443648 efficientnet_model.py:144] round_filter input=16 output=16\n",
            "I0415 13:10:19.508183 139975360443648 efficientnet_model.py:144] round_filter input=24 output=24\n",
            "I0415 13:10:19.583953 139975360443648 efficientnet_model.py:144] round_filter input=24 output=24\n",
            "I0415 13:10:19.584036 139975360443648 efficientnet_model.py:144] round_filter input=40 output=40\n",
            "I0415 13:10:19.664987 139975360443648 efficientnet_model.py:144] round_filter input=40 output=40\n",
            "I0415 13:10:19.665088 139975360443648 efficientnet_model.py:144] round_filter input=80 output=80\n",
            "I0415 13:10:19.764402 139975360443648 efficientnet_model.py:144] round_filter input=80 output=80\n",
            "I0415 13:10:19.764505 139975360443648 efficientnet_model.py:144] round_filter input=112 output=112\n",
            "I0415 13:10:19.868949 139975360443648 efficientnet_model.py:144] round_filter input=112 output=112\n",
            "I0415 13:10:19.869060 139975360443648 efficientnet_model.py:144] round_filter input=192 output=192\n",
            "I0415 13:10:20.029289 139975360443648 efficientnet_model.py:144] round_filter input=192 output=192\n",
            "I0415 13:10:20.029375 139975360443648 efficientnet_model.py:144] round_filter input=320 output=320\n",
            "I0415 13:10:20.074444 139975360443648 efficientnet_model.py:144] round_filter input=1280 output=1280\n",
            "I0415 13:10:20.094682 139975360443648 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0415 13:10:20.116583 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
            "I0415 13:10:20.116692 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 88\n",
            "I0415 13:10:20.116716 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 4\n",
            "I0415 13:10:20.117529 139975360443648 efficientnet_model.py:144] round_filter input=32 output=32\n",
            "I0415 13:10:20.125733 139975360443648 efficientnet_model.py:144] round_filter input=32 output=32\n",
            "I0415 13:10:20.125848 139975360443648 efficientnet_model.py:144] round_filter input=16 output=16\n",
            "I0415 13:10:20.182733 139975360443648 efficientnet_model.py:144] round_filter input=16 output=16\n",
            "I0415 13:10:20.182820 139975360443648 efficientnet_model.py:144] round_filter input=24 output=24\n",
            "I0415 13:10:20.321023 139975360443648 efficientnet_model.py:144] round_filter input=24 output=24\n",
            "I0415 13:10:20.321106 139975360443648 efficientnet_model.py:144] round_filter input=40 output=40\n",
            "I0415 13:10:20.445589 139975360443648 efficientnet_model.py:144] round_filter input=40 output=40\n",
            "I0415 13:10:20.445696 139975360443648 efficientnet_model.py:144] round_filter input=80 output=80\n",
            "I0415 13:10:20.595344 139975360443648 efficientnet_model.py:144] round_filter input=80 output=80\n",
            "I0415 13:10:20.595437 139975360443648 efficientnet_model.py:144] round_filter input=112 output=112\n",
            "I0415 13:10:20.732331 139975360443648 efficientnet_model.py:144] round_filter input=112 output=112\n",
            "I0415 13:10:20.732441 139975360443648 efficientnet_model.py:144] round_filter input=192 output=192\n",
            "I0415 13:10:20.922198 139975360443648 efficientnet_model.py:144] round_filter input=192 output=192\n",
            "I0415 13:10:20.922304 139975360443648 efficientnet_model.py:144] round_filter input=320 output=320\n",
            "I0415 13:10:21.009594 139975360443648 efficientnet_model.py:144] round_filter input=1280 output=1280\n",
            "I0415 13:10:21.027063 139975360443648 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0415 13:10:21.052603 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b2\n",
            "I0415 13:10:21.052685 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 112\n",
            "I0415 13:10:21.052730 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 5\n",
            "I0415 13:10:21.053434 139975360443648 efficientnet_model.py:144] round_filter input=32 output=32\n",
            "I0415 13:10:21.060280 139975360443648 efficientnet_model.py:144] round_filter input=32 output=32\n",
            "I0415 13:10:21.060358 139975360443648 efficientnet_model.py:144] round_filter input=16 output=16\n",
            "I0415 13:10:21.105411 139975360443648 efficientnet_model.py:144] round_filter input=16 output=16\n",
            "I0415 13:10:21.105487 139975360443648 efficientnet_model.py:144] round_filter input=24 output=24\n",
            "I0415 13:10:21.197226 139975360443648 efficientnet_model.py:144] round_filter input=24 output=24\n",
            "I0415 13:10:21.197317 139975360443648 efficientnet_model.py:144] round_filter input=40 output=48\n",
            "I0415 13:10:21.414601 139975360443648 efficientnet_model.py:144] round_filter input=40 output=48\n",
            "I0415 13:10:21.414677 139975360443648 efficientnet_model.py:144] round_filter input=80 output=88\n",
            "I0415 13:10:21.552723 139975360443648 efficientnet_model.py:144] round_filter input=80 output=88\n",
            "I0415 13:10:21.552800 139975360443648 efficientnet_model.py:144] round_filter input=112 output=120\n",
            "I0415 13:10:21.766593 139975360443648 efficientnet_model.py:144] round_filter input=112 output=120\n",
            "I0415 13:10:21.766695 139975360443648 efficientnet_model.py:144] round_filter input=192 output=208\n",
            "I0415 13:10:22.000546 139975360443648 efficientnet_model.py:144] round_filter input=192 output=208\n",
            "I0415 13:10:22.000628 139975360443648 efficientnet_model.py:144] round_filter input=320 output=352\n",
            "I0415 13:10:22.085444 139975360443648 efficientnet_model.py:144] round_filter input=1280 output=1408\n",
            "I0415 13:10:22.110373 139975360443648 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0415 13:10:22.137200 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b3\n",
            "I0415 13:10:22.137303 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 160\n",
            "I0415 13:10:22.137325 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 6\n",
            "I0415 13:10:22.138010 139975360443648 efficientnet_model.py:144] round_filter input=32 output=40\n",
            "I0415 13:10:22.144408 139975360443648 efficientnet_model.py:144] round_filter input=32 output=40\n",
            "I0415 13:10:22.144477 139975360443648 efficientnet_model.py:144] round_filter input=16 output=24\n",
            "I0415 13:10:22.194257 139975360443648 efficientnet_model.py:144] round_filter input=16 output=24\n",
            "I0415 13:10:22.194334 139975360443648 efficientnet_model.py:144] round_filter input=24 output=32\n",
            "I0415 13:10:22.292860 139975360443648 efficientnet_model.py:144] round_filter input=24 output=32\n",
            "I0415 13:10:22.292939 139975360443648 efficientnet_model.py:144] round_filter input=40 output=48\n",
            "I0415 13:10:22.417122 139975360443648 efficientnet_model.py:144] round_filter input=40 output=48\n",
            "I0415 13:10:22.417203 139975360443648 efficientnet_model.py:144] round_filter input=80 output=96\n",
            "I0415 13:10:22.634416 139975360443648 efficientnet_model.py:144] round_filter input=80 output=96\n",
            "I0415 13:10:22.634510 139975360443648 efficientnet_model.py:144] round_filter input=112 output=136\n",
            "I0415 13:10:22.868819 139975360443648 efficientnet_model.py:144] round_filter input=112 output=136\n",
            "I0415 13:10:22.868925 139975360443648 efficientnet_model.py:144] round_filter input=192 output=232\n",
            "I0415 13:10:23.131386 139975360443648 efficientnet_model.py:144] round_filter input=192 output=232\n",
            "I0415 13:10:23.131467 139975360443648 efficientnet_model.py:144] round_filter input=320 output=384\n",
            "I0415 13:10:23.209159 139975360443648 efficientnet_model.py:144] round_filter input=1280 output=1536\n",
            "I0415 13:10:23.227706 139975360443648 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0415 13:10:23.254108 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
            "I0415 13:10:23.254187 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 224\n",
            "I0415 13:10:23.254232 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 7\n",
            "I0415 13:10:23.254912 139975360443648 efficientnet_model.py:144] round_filter input=32 output=48\n",
            "I0415 13:10:23.261184 139975360443648 efficientnet_model.py:144] round_filter input=32 output=48\n",
            "I0415 13:10:23.261248 139975360443648 efficientnet_model.py:144] round_filter input=16 output=24\n",
            "I0415 13:10:23.308998 139975360443648 efficientnet_model.py:144] round_filter input=16 output=24\n",
            "I0415 13:10:23.309079 139975360443648 efficientnet_model.py:144] round_filter input=24 output=32\n",
            "I0415 13:10:23.444687 139975360443648 efficientnet_model.py:144] round_filter input=24 output=32\n",
            "I0415 13:10:23.444760 139975360443648 efficientnet_model.py:144] round_filter input=40 output=56\n",
            "I0415 13:10:23.594730 139975360443648 efficientnet_model.py:144] round_filter input=40 output=56\n",
            "I0415 13:10:23.594808 139975360443648 efficientnet_model.py:144] round_filter input=80 output=112\n",
            "I0415 13:10:23.936928 139975360443648 efficientnet_model.py:144] round_filter input=80 output=112\n",
            "I0415 13:10:23.937052 139975360443648 efficientnet_model.py:144] round_filter input=112 output=160\n",
            "I0415 13:10:24.197104 139975360443648 efficientnet_model.py:144] round_filter input=112 output=160\n",
            "I0415 13:10:24.197183 139975360443648 efficientnet_model.py:144] round_filter input=192 output=272\n",
            "I0415 13:10:24.507080 139975360443648 efficientnet_model.py:144] round_filter input=192 output=272\n",
            "I0415 13:10:24.507175 139975360443648 efficientnet_model.py:144] round_filter input=320 output=448\n",
            "I0415 13:10:24.596902 139975360443648 efficientnet_model.py:144] round_filter input=1280 output=1792\n",
            "I0415 13:10:24.617010 139975360443648 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0415 13:10:24.647886 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
            "I0415 13:10:24.647970 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 288\n",
            "I0415 13:10:24.648016 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 7\n",
            "I0415 13:10:24.648700 139975360443648 efficientnet_model.py:144] round_filter input=32 output=48\n",
            "I0415 13:10:24.654866 139975360443648 efficientnet_model.py:144] round_filter input=32 output=48\n",
            "I0415 13:10:24.654948 139975360443648 efficientnet_model.py:144] round_filter input=16 output=24\n",
            "I0415 13:10:24.728714 139975360443648 efficientnet_model.py:144] round_filter input=16 output=24\n",
            "I0415 13:10:24.728798 139975360443648 efficientnet_model.py:144] round_filter input=24 output=40\n",
            "I0415 13:10:24.904504 139975360443648 efficientnet_model.py:144] round_filter input=24 output=40\n",
            "I0415 13:10:24.904584 139975360443648 efficientnet_model.py:144] round_filter input=40 output=64\n",
            "I0415 13:10:25.102773 139975360443648 efficientnet_model.py:144] round_filter input=40 output=64\n",
            "I0415 13:10:25.102860 139975360443648 efficientnet_model.py:144] round_filter input=80 output=128\n",
            "I0415 13:10:25.370421 139975360443648 efficientnet_model.py:144] round_filter input=80 output=128\n",
            "I0415 13:10:25.370495 139975360443648 efficientnet_model.py:144] round_filter input=112 output=176\n",
            "I0415 13:10:25.626548 139975360443648 efficientnet_model.py:144] round_filter input=112 output=176\n",
            "I0415 13:10:25.626652 139975360443648 efficientnet_model.py:144] round_filter input=192 output=304\n",
            "I0415 13:10:26.045364 139975360443648 efficientnet_model.py:144] round_filter input=192 output=304\n",
            "I0415 13:10:26.045447 139975360443648 efficientnet_model.py:144] round_filter input=320 output=512\n",
            "I0415 13:10:26.211663 139975360443648 efficientnet_model.py:144] round_filter input=1280 output=2048\n",
            "I0415 13:10:26.236151 139975360443648 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0415 13:10:26.275443 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
            "I0415 13:10:26.275523 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 384\n",
            "I0415 13:10:26.275567 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 8\n",
            "I0415 13:10:26.276264 139975360443648 efficientnet_model.py:144] round_filter input=32 output=56\n",
            "I0415 13:10:26.283100 139975360443648 efficientnet_model.py:144] round_filter input=32 output=56\n",
            "I0415 13:10:26.283179 139975360443648 efficientnet_model.py:144] round_filter input=16 output=32\n",
            "I0415 13:10:26.494024 139975360443648 efficientnet_model.py:144] round_filter input=16 output=32\n",
            "I0415 13:10:26.494128 139975360443648 efficientnet_model.py:144] round_filter input=24 output=40\n",
            "I0415 13:10:26.700505 139975360443648 efficientnet_model.py:144] round_filter input=24 output=40\n",
            "I0415 13:10:26.700608 139975360443648 efficientnet_model.py:144] round_filter input=40 output=72\n",
            "I0415 13:10:26.950586 139975360443648 efficientnet_model.py:144] round_filter input=40 output=72\n",
            "I0415 13:10:26.950689 139975360443648 efficientnet_model.py:144] round_filter input=80 output=144\n",
            "I0415 13:10:27.251111 139975360443648 efficientnet_model.py:144] round_filter input=80 output=144\n",
            "I0415 13:10:27.251194 139975360443648 efficientnet_model.py:144] round_filter input=112 output=200\n",
            "I0415 13:10:27.537718 139975360443648 efficientnet_model.py:144] round_filter input=112 output=200\n",
            "I0415 13:10:27.537816 139975360443648 efficientnet_model.py:144] round_filter input=192 output=344\n",
            "I0415 13:10:28.037737 139975360443648 efficientnet_model.py:144] round_filter input=192 output=344\n",
            "I0415 13:10:28.037826 139975360443648 efficientnet_model.py:144] round_filter input=320 output=576\n",
            "I0415 13:10:28.210075 139975360443648 efficientnet_model.py:144] round_filter input=1280 output=2304\n",
            "I0415 13:10:28.233142 139975360443648 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0415 13:10:28.277783 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b7\n",
            "I0415 13:10:28.277873 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 384\n",
            "I0415 13:10:28.277899 139975360443648 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 8\n",
            "I0415 13:10:28.278665 139975360443648 efficientnet_model.py:144] round_filter input=32 output=64\n",
            "I0415 13:10:28.284888 139975360443648 efficientnet_model.py:144] round_filter input=32 output=64\n",
            "I0415 13:10:28.284942 139975360443648 efficientnet_model.py:144] round_filter input=16 output=32\n",
            "I0415 13:10:28.387530 139975360443648 efficientnet_model.py:144] round_filter input=16 output=32\n",
            "I0415 13:10:28.387633 139975360443648 efficientnet_model.py:144] round_filter input=24 output=48\n",
            "I0415 13:10:28.651899 139975360443648 efficientnet_model.py:144] round_filter input=24 output=48\n",
            "I0415 13:10:28.651984 139975360443648 efficientnet_model.py:144] round_filter input=40 output=80\n",
            "I0415 13:10:28.948695 139975360443648 efficientnet_model.py:144] round_filter input=40 output=80\n",
            "I0415 13:10:28.948776 139975360443648 efficientnet_model.py:144] round_filter input=80 output=160\n",
            "I0415 13:10:29.491051 139975360443648 efficientnet_model.py:144] round_filter input=80 output=160\n",
            "I0415 13:10:29.491135 139975360443648 efficientnet_model.py:144] round_filter input=112 output=224\n",
            "I0415 13:10:29.893677 139975360443648 efficientnet_model.py:144] round_filter input=112 output=224\n",
            "I0415 13:10:29.893794 139975360443648 efficientnet_model.py:144] round_filter input=192 output=384\n",
            "I0415 13:10:30.461786 139975360443648 efficientnet_model.py:144] round_filter input=192 output=384\n",
            "I0415 13:10:30.461869 139975360443648 efficientnet_model.py:144] round_filter input=320 output=640\n",
            "I0415 13:10:30.711249 139975360443648 efficientnet_model.py:144] round_filter input=1280 output=2560\n",
            "I0415 13:10:30.742360 139975360443648 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 11.49s\n",
            "I0415 13:10:30.790515 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 11.49s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "I0415 13:10:30.795729 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "I0415 13:10:30.796556 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "I0415 13:10:30.796778 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "I0415 13:10:30.797358 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTF2Test.test_session\n",
            "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "I0415 13:10:30.797885 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "I0415 13:10:30.798064 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "I0415 13:10:30.798441 139975360443648 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 24 tests in 13.434s\n",
            "\n",
            "OK (skipped=1)\n"
          ]
        }
      ],
      "source": [
        "# Run Model Bulider Test file, just to verify everything's working properly\n",
        "!python models/research/object_detection/builders/model_builder_tf2_test.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eydREUsMGUUR"
      },
      "source": [
        "# 4.&nbsp;Upload Image Dataset and Prepare Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSZVCxE4nSVI"
      },
      "source": [
        "In this section, we'll upload our data and prepare it for training with TensorFlow. We'll upload our images, split them into train, validation, and test folders, and then run scripts for creating TFRecords from our data.\n",
        "\n",
        "First, on your local PC, zip all your training images and XML files into a single folder called \"images.zip\". The files should be directly inside the zip folder, or in a nested folder as shown below:\n",
        "```\n",
        "images.zip\n",
        "-- images\n",
        "  -- img1.jpg\n",
        "  -- img1.xml\n",
        "  -- img2.jpg\n",
        "  -- img2.xml\n",
        "  ...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE1MtX4HGQA4"
      },
      "source": [
        "### 4.1 Upload images\n",
        "There are three options for moving the image files to this Colab instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFSJoDEnJotN"
      },
      "source": [
        "**Option 1. Upload your custom dataset**\n",
        "\n",
        "Upload your custom dataset containing the images and annotation (.xml) to your project folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xAJMKwpFilm"
      },
      "source": [
        "**Option 2. Use pre provided dataset**\n",
        "\n",
        "If you don't have a dataset and just want to try training a model, you can download a Toradex modules dataset to use as example. This dataset contains 185 labeled images of Toradex SoMs.Run the following code block to download the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suu_xPVZIEcH"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/lucasbernardestoradex/voc_dataset/raw/main/only_som.zip # only som dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHjOhoSGYwT7"
      },
      "source": [
        "## 4.2 Create Labelmap\n",
        "At this point, whether you used Option 1 or 2, you should be able to click the folder icon on the left and see your \"images.zip\" file in the list of files. Now that the dataset is uploaded, let's unzip it and create some folders to hold the images. These directories are created in the current folder in this instance's filesystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mGvoHH-unSVO"
      },
      "outputs": [],
      "source": [
        "!mkdir images\n",
        "!unzip -q only_som.zip -d images/all\n",
        "!mkdir images/train; mkdir images/validation; mkdir images/test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To manually add your labels, replace \"class1\", \"class2\", and \"class3\" with the labels from your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### This creates a a \"labelmap.txt\" file with a list of classes the object detection model will detect.\n",
        "aux = \"\"\"class1\n",
        "class2\n",
        "class3\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The labels below correspond to the \"only_som\" dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### This creates a a \"labelmap.txt\" file with a list of classes the object detection model will detect for the \"only_som dataset\"\n",
        "\n",
        "aux = \"\"\"som\"\"\"\n",
        "with open('labelmap.txt', 'w') as f:\n",
        "  f.write(aux)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also use the script below to automatically capture the labels from your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "!./script_take_labels.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p--K1PJXEgNo"
      },
      "source": [
        "## 4.3 Split images into train, validation, and test folders and Create TFRecords\n",
        "Finally, we need to create a labelmap for the detector and convert the images into a data file format called TFRecords, which are used by TensorFlow for training. We'll use Python scripts to automatically convert the data into TFRecord format. Before running them, we need to define a labelmap for our classes.\n",
        "\n",
        "The code section below will create a \"labelmap.txt\" file that contains a list of classes. Replace the `class1`, `class2`, `class3` text with your own classes (for example, `penny`, `nickel`, `dime`, `quarter`), adding a new line for each class. Then, click play to execute the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pa2VYhTIT1l"
      },
      "source": [
        "Download and run the data conversion scripts from the [GitHub repository](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi) by clicking play on the following three sections of code. They will create TFRecord files for the train and validation datasets, as well as a `labelmap.pbtxt` file which contains the labelmap in a different format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-6RIcrwbQMh"
      },
      "source": [
        "Next, we'll split the images into train, validation, and test sets. Here's what each set is used for:\n",
        "\n",
        "\n",
        "\n",
        "*   **Train**: These are the actual images used to train the model. In each step of training, a batch of images from the \"train\" set is passed into the neural network. The network predicts classes and locations of objects in the images. The training algorithm calculates the loss (i.e. how \"wrong\" the predictions were) and adjusts the network weights through backpropagation.\n",
        "\n",
        "\n",
        "*   **Validation**: Images from the \"validation\" set can be used by the training algorithm to check the progress of training and adjust hyperparameters (like learning rate). Unlike \"train\" images, these images are only used periodically during training (i.e. once every certain number of training steps).\n",
        "\n",
        "\n",
        "* **Test**: These images are never seen by the neural network during training. They are intended to be used by a human to perform final testing of the model to check how accurate the model is.\n",
        "\n",
        "I wrote a Python script to randomly move 80% of the images to the \"train\" folder, 10% to the \"validation\" folder, and 10% to the \"test\" folder. Click play on the following block to download the script and execute it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total images: 185\n",
            "Images moving to train: 148\n",
            "Images moving to validation: 18\n",
            "Images moving to test: 19\n"
          ]
        }
      ],
      "source": [
        "# !wget https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/train_val_test_split.py\n",
        "!python train_val_test_split.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "laZZE0TlEeUF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-04-15 13:17:10--  https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/create_csv.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1348 (1.3K) [text/plain]\n",
            "Saving to: ‘create_csv.py’\n",
            "\n",
            "create_csv.py       100%[===================>]   1.32K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-15 13:17:11 (54.7 MB/s) - ‘create_csv.py’ saved [1348/1348]\n",
            "\n",
            "--2024-04-15 13:17:11--  https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/create_tfrecord.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4414 (4.3K) [text/plain]\n",
            "Saving to: ‘create_tfrecord.py’\n",
            "\n",
            "create_tfrecord.py  100%[===================>]   4.31K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-15 13:17:11 (12.1 MB/s) - ‘create_tfrecord.py’ saved [4414/4414]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download data conversion scripts\n",
        "! wget https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/create_csv.py\n",
        "! wget https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/create_tfrecord.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5tdDbTmHYwu-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully converted xml to csv.\n",
            "Successfully converted xml to csv.\n",
            "2024-04-15 13:17:21.656124: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2024-04-15 13:17:21.656170: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (lucasb-nb): /proc/driver/nvidia/version does not exist\n",
            "Successfully created the TFRecords: /home/lucas_bernardes/Documents/computer_vision/TrainingTFLiteToradex/train.tfrecord\n",
            "2024-04-15 13:17:28.947979: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2024-04-15 13:17:28.948020: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (lucasb-nb): /proc/driver/nvidia/version does not exist\n",
            "Successfully created the TFRecords: /home/lucas_bernardes/Documents/computer_vision/TrainingTFLiteToradex/val.tfrecord\n"
          ]
        }
      ],
      "source": [
        "# Create CSV data files and TFRecord files\n",
        "!python3 create_csv.py\n",
        "!python3 create_tfrecord.py --csv_input=images/train_labels.csv --labelmap=labelmap.txt --image_dir=images/train --output_path=train.tfrecord\n",
        "!python3 create_tfrecord.py --csv_input=images/validation_labels.csv --labelmap=labelmap.txt --image_dir=images/validation --output_path=val.tfrecord"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNyv_YyDXwMs"
      },
      "source": [
        "We'll store the locations of the TFRecord and labelmap files as variables so we can reference them later in this Colab session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YUd2wtfrqedy"
      },
      "outputs": [],
      "source": [
        "train_record_fname = 'train.tfrecord'\n",
        "val_record_fname = 'val.tfrecord'\n",
        "label_map_pbtxt_fname = 'labelmap.pbtxt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGEUZYAMEZ6f"
      },
      "source": [
        "# 5.&nbsp;Set Up Training Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2MAcgJ53STW"
      },
      "source": [
        "In this section, we'll set up the model and training configuration. We'll specifiy which pretrained TensorFlow model we want to use from the [TensorFlow 2 Object Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). Each model also comes with a configuration file that points to file locations, sets training parameters (such as learning rate and total number of training steps), and more. We'll modify the configuration file for our custom training job.\n",
        "\n",
        "The first section of code lists out some models availabe in the TF2 Model Zoo and defines some filenames that will be used later to download the model and config file. This makes it easy to manage which model you're using and to add other models to the list later.\n",
        "\n",
        "Set the \"chosen_model\" variable to match the name of the model you'd like to train with. It's currently set to use the popular \"ssd-mobilenet-v2\" model. Click play on the next block once the chosen model has been set.\n",
        "\n",
        "Not sure which model to pick? [Check out my blog post comparing each model's speed and accuracy.](https://ejtech.io/learn/tflite-object-detection-model-comparison)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gN0EUEa3e5Un"
      },
      "outputs": [],
      "source": [
        "# Change the chosen_model variable to deploy different models available in the TF2 object detection zoo\n",
        "chosen_model = 'ssd-mobilenet-v2-fpnlite-320'\n",
        "\n",
        "MODELS_CONFIG = {\n",
        "    'ssd-mobilenet-v2': {\n",
        "        'model_name': 'ssd_mobilenet_v2_320x320_coco17_tpu-8',\n",
        "        'base_pipeline_file': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz',\n",
        "    },\n",
        "    'efficientdet-d0': {\n",
        "        'model_name': 'efficientdet_d0_coco17_tpu-32',\n",
        "        'base_pipeline_file': 'ssd_efficientdet_d0_512x512_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'efficientdet_d0_coco17_tpu-32.tar.gz',\n",
        "    },\n",
        "    'ssd-mobilenet-v2-fpnlite-320': {\n",
        "        'model_name': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8',\n",
        "        'base_pipeline_file': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz',\n",
        "    },\n",
        "    # The centernet model isn't working as of 9/10/22\n",
        "    #'centernet-mobilenet-v2': {\n",
        "    #    'model_name': 'centernet_mobilenetv2fpn_512x512_coco17_od',\n",
        "    #    'base_pipeline_file': 'pipeline.config',\n",
        "    #    'pretrained_checkpoint': 'centernet_mobilenetv2fpn_512x512_coco17_od.tar.gz',\n",
        "    #}\n",
        "}\n",
        "\n",
        "model_name = MODELS_CONFIG[chosen_model]['model_name']\n",
        "pretrained_checkpoint = MODELS_CONFIG[chosen_model]['pretrained_checkpoint']\n",
        "base_pipeline_file = MODELS_CONFIG[chosen_model]['base_pipeline_file']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMG3EEPqPggV"
      },
      "source": [
        "Download the pretrained model file and configuration file by clicking Play on the following section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kG4TmJUVrYQ7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
            "--2024-04-15 13:17:33--  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 142.250.219.155, 142.251.129.187, 142.251.132.59, ...\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|142.250.219.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20515344 (20M) [application/x-tar]\n",
            "Saving to: ‘models/mymodel/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz’\n",
            "\n",
            "ssd_mobilenet_v2_fp 100%[===================>]  19.56M  9.91MB/s    in 2.0s    \n",
            "\n",
            "2024-04-15 13:17:36 (9.91 MB/s) - ‘models/mymodel/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz’ saved [20515344/20515344]\n",
            "\n",
            "--2024-04-15 13:17:36--  https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4684 (4.6K) [text/plain]\n",
            "Saving to: ‘models/mymodel/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config’\n",
            "\n",
            "ssd_mobilenet_v2_fp 100%[===================>]   4.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-15 13:17:37 (29.1 MB/s) - ‘models/mymodel/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config’ saved [4684/4684]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create \"mymodel\" folder for holding pre-trained weights and configuration files\n",
        "!mkdir models/mymodel/\n",
        "\n",
        "# Download pre-trained model weights\n",
        "import tarfile\n",
        "download_tar = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/' + pretrained_checkpoint\n",
        "print(download_tar)\n",
        "!wget -P models/mymodel {download_tar}\n",
        "tar = tarfile.open('models/mymodel/' + pretrained_checkpoint)\n",
        "tar.extractall(path='models/mymodel')\n",
        "tar.close()\n",
        "\n",
        "# Download training configuration file for model\n",
        "download_config = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/' + base_pipeline_file\n",
        "!wget -P models/mymodel {download_config}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFAlqNrPn5y3"
      },
      "source": [
        "Now that we've downloaded our model and config file, we need to modify the configuration file with some high-level training parameters. The following variables are used to control training steps:\n",
        "\n",
        "* **num_steps**: The total amount of steps to use for training the model. A good number to start with is 40,000 steps. You can use more steps if you notice the loss metrics are still decreasing by the time training finishes. The more steps, the longer training will take. Training can also be stopped early if loss flattens out before reaching the specified number of steps.\n",
        "* **batch_size**: The number of images to use per training step. A larger batch size allows a model to be trained in fewer steps, but the size is limited by the GPU memory available for training. With the GPUs used in Colab instances, 16 is a good number for SSD models and 4 is good for EfficientDet models.\n",
        "\n",
        "Other training information, like the location of the pretrained model file, the config file, and total number of classes are also assigned in this step. To learn more about training configuration with the TensorFlow Object Detection API, read this [article from Neptune](https://neptune.ai/blog/tensorflow-object-detection-api-best-practices-to-training-evaluation-deployment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1lYDvJN-n69v"
      },
      "outputs": [],
      "source": [
        "# Set training parameters for the model\n",
        "num_steps = 40000\n",
        "# num_steps = 100\n",
        "\n",
        "if chosen_model == 'efficientdet-d0':\n",
        "  batch_size = 4\n",
        "else:\n",
        "  # batch_size = 4\n",
        "  batch_size = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "b_ki9jOqxn7V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total classes: 1\n"
          ]
        }
      ],
      "source": [
        "# Set file locations and get number of classes for config file\n",
        "pipeline_fname = 'models/mymodel/' + base_pipeline_file\n",
        "fine_tune_checkpoint = 'models/mymodel/' + model_name + '/checkpoint/ckpt-0'\n",
        "\n",
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
        "print('Total classes:', num_classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwPyaIAXxyKu"
      },
      "source": [
        "Next, we'll rewrite the config file to use the training parameters we just specified. The following section of code will automatically replace the necessary parameters in the downloaded .config file and save it as our custom \"pipeline_file.config\" file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5eA5ht3_yukT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "writing custom configuration file\n"
          ]
        }
      ],
      "source": [
        "# Create custom configuration file by writing the dataset, model checkpoint, and training parameters into the base pipeline file\n",
        "import re\n",
        "\n",
        "print('writing custom configuration file')\n",
        "\n",
        "with open(pipeline_fname) as f:\n",
        "    s = f.read()\n",
        "with open('models/mymodel/pipeline_file.config', 'w') as f:\n",
        "\n",
        "    # Set fine_tune_checkpoint path\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
        "\n",
        "    # Set tfrecord files for train and test datasets\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', 'input_path: \"{}\"'.format(val_record_fname), s)\n",
        "\n",
        "    # Set label_map_path\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "\n",
        "    # Set batch_size\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    # Set training steps, num_steps\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "\n",
        "    # Set number of classes num_classes\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "\n",
        "    # Change fine-tune checkpoint type from \"classification\" to \"detection\"\n",
        "    s = re.sub(\n",
        "        'fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n",
        "\n",
        "    # If using ssd-mobilenet-v2, reduce learning rate (because it's too high in the default config file)\n",
        "    if chosen_model == 'ssd-mobilenet-v2':\n",
        "      s = re.sub('learning_rate_base: .8',\n",
        "                 'learning_rate_base: .08', s)\n",
        "\n",
        "      s = re.sub('warmup_learning_rate: 0.13333',\n",
        "                 'warmup_learning_rate: .026666', s)\n",
        "\n",
        "    # If using efficientdet-d0, use fixed_shape_resizer instead of keep_aspect_ratio_resizer (because it isn't supported by TFLite)\n",
        "    if chosen_model == 'efficientdet-d0':\n",
        "      s = re.sub('keep_aspect_ratio_resizer', 'fixed_shape_resizer', s)\n",
        "      s = re.sub('pad_to_max_dimension: true', '', s)\n",
        "      s = re.sub('min_dimension', 'height', s)\n",
        "      s = re.sub('max_dimension', 'width', s)\n",
        "\n",
        "    f.write(s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDySP7TLzdCM"
      },
      "source": [
        "(Optional) If you're curious, you can display the configuration file's contents here in the browser by running the line of code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HEsOLOMHzBqF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# SSD with Mobilenet v2 FPN-lite (go/fpn-lite) feature extractor, shared box\n",
            "# predictor and focal loss (a mobile version of Retinanet).\n",
            "# Retinanet: see Lin et al, https://arxiv.org/abs/1708.02002\n",
            "# Trained on COCO, initialized from Imagenet classification checkpoint\n",
            "# Train on TPU-8\n",
            "#\n",
            "# Achieves 22.2 mAP on COCO17 Val\n",
            "\n",
            "model {\n",
            "  ssd {\n",
            "    inplace_batchnorm_update: true\n",
            "    freeze_batchnorm: false\n",
            "    num_classes: 1\n",
            "    box_coder {\n",
            "      faster_rcnn_box_coder {\n",
            "        y_scale: 10.0\n",
            "        x_scale: 10.0\n",
            "        height_scale: 5.0\n",
            "        width_scale: 5.0\n",
            "      }\n",
            "    }\n",
            "    matcher {\n",
            "      argmax_matcher {\n",
            "        matched_threshold: 0.5\n",
            "        unmatched_threshold: 0.5\n",
            "        ignore_thresholds: false\n",
            "        negatives_lower_than_unmatched: true\n",
            "        force_match_for_each_row: true\n",
            "        use_matmul_gather: true\n",
            "      }\n",
            "    }\n",
            "    similarity_calculator {\n",
            "      iou_similarity {\n",
            "      }\n",
            "    }\n",
            "    encode_background_as_zeros: true\n",
            "    anchor_generator {\n",
            "      multiscale_anchor_generator {\n",
            "        min_level: 3\n",
            "        max_level: 7\n",
            "        anchor_scale: 4.0\n",
            "        aspect_ratios: [1.0, 2.0, 0.5]\n",
            "        scales_per_octave: 2\n",
            "      }\n",
            "    }\n",
            "    image_resizer {\n",
            "      fixed_shape_resizer {\n",
            "        height: 320\n",
            "        width: 320\n",
            "      }\n",
            "    }\n",
            "    box_predictor {\n",
            "      weight_shared_convolutional_box_predictor {\n",
            "        depth: 128\n",
            "        class_prediction_bias_init: -4.6\n",
            "        conv_hyperparams {\n",
            "          activation: RELU_6,\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 0.00004\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            random_normal_initializer {\n",
            "              stddev: 0.01\n",
            "              mean: 0.0\n",
            "            }\n",
            "          }\n",
            "          batch_norm {\n",
            "            scale: true,\n",
            "            decay: 0.997,\n",
            "            epsilon: 0.001,\n",
            "          }\n",
            "        }\n",
            "        num_layers_before_predictor: 4\n",
            "        share_prediction_tower: true\n",
            "        use_depthwise: true\n",
            "        kernel_size: 3\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: 'ssd_mobilenet_v2_fpn_keras'\n",
            "      use_depthwise: true\n",
            "      fpn {\n",
            "        min_level: 3\n",
            "        max_level: 7\n",
            "        additional_layer_depth: 128\n",
            "      }\n",
            "      min_depth: 16\n",
            "      depth_multiplier: 1.0\n",
            "      conv_hyperparams {\n",
            "        activation: RELU_6,\n",
            "        regularizer {\n",
            "          l2_regularizer {\n",
            "            weight: 0.00004\n",
            "          }\n",
            "        }\n",
            "        initializer {\n",
            "          random_normal_initializer {\n",
            "            stddev: 0.01\n",
            "            mean: 0.0\n",
            "          }\n",
            "        }\n",
            "        batch_norm {\n",
            "          scale: true,\n",
            "          decay: 0.997,\n",
            "          epsilon: 0.001,\n",
            "        }\n",
            "      }\n",
            "      override_base_feature_extractor_hyperparams: true\n",
            "    }\n",
            "    loss {\n",
            "      classification_loss {\n",
            "        weighted_sigmoid_focal {\n",
            "          alpha: 0.25\n",
            "          gamma: 2.0\n",
            "        }\n",
            "      }\n",
            "      localization_loss {\n",
            "        weighted_smooth_l1 {\n",
            "        }\n",
            "      }\n",
            "      classification_weight: 1.0\n",
            "      localization_weight: 1.0\n",
            "    }\n",
            "    normalize_loss_by_num_matches: true\n",
            "    normalize_loc_loss_by_codesize: true\n",
            "    post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 1e-8\n",
            "        iou_threshold: 0.6\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 100\n",
            "      }\n",
            "      score_converter: SIGMOID\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_config: {\n",
            "  fine_tune_checkpoint_version: V2\n",
            "  fine_tune_checkpoint: \"models/mymodel/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0\"\n",
            "  fine_tune_checkpoint_type: \"detection\"\n",
            "  batch_size: 16\n",
            "  sync_replicas: true\n",
            "  startup_delay_steps: 0\n",
            "  replicas_to_aggregate: 8\n",
            "  num_steps: 40000\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "  data_augmentation_options {\n",
            "    random_crop_image {\n",
            "      min_object_covered: 0.0\n",
            "      min_aspect_ratio: 0.75\n",
            "      max_aspect_ratio: 3.0\n",
            "      min_area: 0.75\n",
            "      max_area: 1.0\n",
            "      overlap_thresh: 0.0\n",
            "    }\n",
            "  }\n",
            "  optimizer {\n",
            "    momentum_optimizer: {\n",
            "      learning_rate: {\n",
            "        cosine_decay_learning_rate {\n",
            "          learning_rate_base: .08\n",
            "          total_steps: 50000\n",
            "          warmup_learning_rate: .026666\n",
            "          warmup_steps: 1000\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.9\n",
            "    }\n",
            "    use_moving_average: false\n",
            "  }\n",
            "  max_number_of_boxes: 100\n",
            "  unpad_groundtruth_tensors: false\n",
            "}\n",
            "\n",
            "train_input_reader: {\n",
            "  label_map_path: \"labelmap.pbtxt\"\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"train.tfrecord\"\n",
            "  }\n",
            "}\n",
            "\n",
            "eval_config: {\n",
            "  metrics_set: \"coco_detection_metrics\"\n",
            "  use_moving_averages: false\n",
            "}\n",
            "\n",
            "eval_input_reader: {\n",
            "  label_map_path: \"labelmap.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_epochs: 1\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"val.tfrecord\"\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# (Optional) Display the custom configuration file's contents\n",
        "!cat models/mymodel/pipeline_file.config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXpnXYC908Zl"
      },
      "source": [
        "Finally, let's set the locations of the configuration file and model output directory as variables so we can reference them when we call the training command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GMlaN3rs3zLe"
      },
      "outputs": [],
      "source": [
        "# Set the path to the custom config file and the directory to store training checkpoints in\n",
        "pipeline_file = 'models/mymodel/pipeline_file.config'\n",
        "model_dir = 'training/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-19zML6oEO7l"
      },
      "source": [
        "# 6.&nbsp;Train Custom TFLite Detection Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxPj_QV43qD5"
      },
      "source": [
        "We're ready to train our object detection model! Before we start training, let's load up a TensorBoard session to monitor training progress. Run the following section of code, and a TensorBoard session will appear in the browser. It won't show anything yet, because we haven't started training. Once training starts, come back and click the refresh button to see the model's overall loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboard in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorboard) (1.62.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorboard) (2.29.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorboard) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorboard) (3.6)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorboard) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorboard) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorboard) (68.2.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorboard) (3.0.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from tensorboard) (0.41.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (2.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard) (7.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from werkzeug>=0.11.15->tensorboard) (2.1.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.18.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /home/lucas_bernardes/miniconda3/envs/test_presentation/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To see the tensorboard, type `http://localhost:8888/` on your browser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-bcab8560e98c878e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-bcab8560e98c878e\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 8888;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir 'training/train' --host localhost --port 8888"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cuQpPJL2pUq"
      },
      "source": [
        "Model training is performed using the \"model_main_tf2.py\" script from the TF Object Detection API. Training will take anywhere from 2 to 6 hours, depending on the model, batch size, and number of training steps. We've already defined all the parameters and arguments used by `model_main_tf2.py` in previous sections of this Colab. Just click Play on the following block to begin training!\n",
        "\n",
        "\n",
        "\n",
        "> *Note: It takes a few minutes for the program to display any training messages, because it only displays logs once every 100 steps. If it seems like nothing is happening, just wait a couple minutes.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQTfZChVzzpZ"
      },
      "outputs": [],
      "source": [
        "# Run training!\n",
        "!python models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path={pipeline_file} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --sample_1_of_n_eval_examples=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHxbX4ZpzXIv"
      },
      "source": [
        "If you want to stop training early, just click Stop a couple times or right-click on the code block and select \"Interrupt Execution\". Otherwise, training will stop by itself once it reaches the specified number of training steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPg8oMnQDYKl"
      },
      "source": [
        "# 7.&nbsp;Convert Model to TensorFlow Lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spQXdq8Y63pj"
      },
      "source": [
        "Alright! Our model is all trained up and ready to be used for detecting objects. First, we need to export the model graph (a file that contains information about the architecture and weights) to a TensorFlow Lite-compatible format. We'll do this using the `export_tflite_graph_tf2.py` script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaUU8tBlHifd"
      },
      "outputs": [],
      "source": [
        "# Make a directory to store the trained TFLite model\n",
        "!mkdir custom_model_lite\n",
        "output_directory = 'custom_model_lite'\n",
        "\n",
        "# Path to training directory (the conversion script automatically chooses the highest checkpoint file)\n",
        "last_model_path = 'training'\n",
        "\n",
        "!python models/research/object_detection/export_tflite_graph_tf2.py \\\n",
        "    --trained_checkpoint_dir {last_model_path} \\\n",
        "    --output_directory {output_directory} \\\n",
        "    --pipeline_config_path {pipeline_file}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_NuapO2VROu"
      },
      "source": [
        "Next, we'll take the exported graph and use the `TFLiteConverter` module to convert it to `.tflite` FlatBuffer format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsE_uVjlsz3u"
      },
      "outputs": [],
      "source": [
        "# Convert exported graph file into TFLite model file\n",
        "import tensorflow as tf\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('custom_model_lite/saved_model')\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('custom_model_lite/detect.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDQrtQhvC3oG"
      },
      "source": [
        "# 8.&nbsp;Test TensorFlow Lite Model and Calculate mAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtSmUZcxIAvt"
      },
      "source": [
        "We've trained our custom model and converted it to TFLite format. But how well does it actually perform at detecting objects in images? This is where the images we set aside in the **test** folder come in. The model never saw any test images during training, so its performance on these images should be representative of how it will perform on new images from the field.\n",
        "\n",
        "### 8.1 Inference test images\n",
        "The following code defines a function to run inference on test images. It loads the images, loads the model and labelmap, runs the model on each image, and displays the result. It also optionally saves detection results as text files so we can use them to calculate model mAP score.\n",
        "\n",
        "This code is based off the [TFLite_detection_image.py](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_image.py) script from my [TensorFlow Lite Object Detection repository on GitHub](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi); feel free to use it as a starting point for your own application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4WtI8i5K96w"
      },
      "outputs": [],
      "source": [
        "# Script to run custom TFLite model on test images to detect objects\n",
        "# Source: https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_image.py\n",
        "\n",
        "# Import packages\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import glob\n",
        "import random\n",
        "import importlib.util\n",
        "from tensorflow.lite.python.interpreter import Interpreter\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "### Define function for inferencing with TFLite model and displaying results\n",
        "\n",
        "def tflite_detect_images(modelpath, imgpath, lblpath, min_conf=0.5, num_test_images=10, savepath='results', txt_only=False):\n",
        "\n",
        "  # Grab filenames of all images in test folder\n",
        "  images = glob.glob(imgpath + '/*.jpg') + glob.glob(imgpath + '/*.JPG') + glob.glob(imgpath + '/*.png') + glob.glob(imgpath + '/*.bmp')\n",
        "\n",
        "  # Load the label map into memory\n",
        "  with open(lblpath, 'r') as f:\n",
        "      labels = [line.strip() for line in f.readlines()]\n",
        "\n",
        "  # Load the Tensorflow Lite model into memory\n",
        "  interpreter = Interpreter(model_path=modelpath)\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  # Get model details\n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "  height = input_details[0]['shape'][1]\n",
        "  width = input_details[0]['shape'][2]\n",
        "\n",
        "  float_input = (input_details[0]['dtype'] == np.float32)\n",
        "\n",
        "  input_mean = 127.5\n",
        "  input_std = 127.5\n",
        "\n",
        "  # Randomly select test images\n",
        "  images_to_test = random.sample(images, num_test_images)\n",
        "\n",
        "  # Loop over every image and perform detection\n",
        "  for image_path in images_to_test:\n",
        "\n",
        "      # Load image and resize to expected shape [1xHxWx3]\n",
        "      image = cv2.imread(image_path)\n",
        "      image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "      imH, imW, _ = image.shape\n",
        "      image_resized = cv2.resize(image_rgb, (width, height))\n",
        "      input_data = np.expand_dims(image_resized, axis=0)\n",
        "\n",
        "      # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n",
        "      if float_input:\n",
        "          input_data = (np.float32(input_data) - input_mean) / input_std\n",
        "\n",
        "      # Perform the actual detection by running the model with the image as input\n",
        "      interpreter.set_tensor(input_details[0]['index'],input_data)\n",
        "      interpreter.invoke()\n",
        "\n",
        "      # Retrieve detection results\n",
        "      boxes = interpreter.get_tensor(output_details[1]['index'])[0] # Bounding box coordinates of detected objects\n",
        "      classes = interpreter.get_tensor(output_details[3]['index'])[0] # Class index of detected objects\n",
        "      scores = interpreter.get_tensor(output_details[0]['index'])[0] # Confidence of detected objects\n",
        "\n",
        "      detections = []\n",
        "\n",
        "      # Loop over all detections and draw detection box if confidence is above minimum threshold\n",
        "      for i in range(len(scores)):\n",
        "          if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n",
        "\n",
        "              # Get bounding box coordinates and draw box\n",
        "              # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n",
        "              ymin = int(max(1,(boxes[i][0] * imH)))\n",
        "              xmin = int(max(1,(boxes[i][1] * imW)))\n",
        "              ymax = int(min(imH,(boxes[i][2] * imH)))\n",
        "              xmax = int(min(imW,(boxes[i][3] * imW)))\n",
        "\n",
        "              cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n",
        "\n",
        "              # Draw label\n",
        "              object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\n",
        "              label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\n",
        "              labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n",
        "              label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n",
        "              cv2.rectangle(image, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n",
        "              cv2.putText(image, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n",
        "\n",
        "              detections.append([object_name, scores[i], xmin, ymin, xmax, ymax])\n",
        "\n",
        "\n",
        "      # All the results have been drawn on the image, now display the image\n",
        "      if txt_only == False: # \"text_only\" controls whether we want to display the image results or just save them in .txt files\n",
        "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "        plt.figure(figsize=(12,16))\n",
        "        plt.imshow(image)\n",
        "        plt.show()\n",
        "\n",
        "      # Save detection results in .txt files (for calculating mAP)\n",
        "      elif txt_only == True:\n",
        "\n",
        "        # Get filenames and paths\n",
        "        image_fn = os.path.basename(image_path)\n",
        "        base_fn, ext = os.path.splitext(image_fn)\n",
        "        txt_result_fn = base_fn +'.txt'\n",
        "        txt_savepath = os.path.join(savepath, txt_result_fn)\n",
        "\n",
        "        # Write results to text file\n",
        "        # (Using format defined by https://github.com/Cartucho/mAP, which will make it easy to calculate mAP)\n",
        "        with open(txt_savepath,'w') as f:\n",
        "            for detection in detections:\n",
        "                f.write('%s %.4f %d %d %d %d\\n' % (detection[0], detection[1], detection[2], detection[3], detection[4], detection[5]))\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CJI4A0f_zqz"
      },
      "source": [
        "The next block sets the paths to the test images and models and then runs the inferencing function. If you want to use more than 10 images, change the `images_to_test` variable. Click play to run inferencing!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6t8CMarqBqP9"
      },
      "outputs": [],
      "source": [
        "# Set up variables for running user's model\n",
        "PATH_TO_IMAGES='images/test'   # Path to test images folder\n",
        "PATH_TO_MODEL='custom_model_lite/detect.tflite'   # Path to .tflite model file\n",
        "PATH_TO_LABELS='labelmap.txt'   # Path to labelmap.txt file\n",
        "min_conf_threshold=0.5   # Confidence threshold (try changing this to 0.01 if you don't see any detection results)\n",
        "images_to_test = 10   # Number of images to run detection on\n",
        "\n",
        "# Run inferencing function!\n",
        "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_ckqeWqBF0P"
      },
      "source": [
        "### 8.2 Calculate mAP\n",
        "Now we have a visual sense of how our model performs on test images, but how can we quantitatively measure its accuracy?\n",
        "\n",
        "One popular methord for measuring object detection model accuracy is \"mean average precision\" (mAP). Basically, the higher the mAP score, the better your model is at detecting objects in images. To learn more about mAP, read through this [article from Roboflow](https://blog.roboflow.com/mean-average-precision/).\n",
        "\n",
        "We'll use the mAP calculator tool at https://github.com/Cartucho/mAP to determine our model's mAP score. First, we need to clone the repository and remove its existing example data. We'll also download a script I wrote for interfacing with the calculator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlWarXEZDUqS"
      },
      "outputs": [],
      "source": [
        "!mkdir mAP\n",
        "!git clone https://github.com/Cartucho/mAP mAP\n",
        "!rm mAP/input/detection-results/*\n",
        "!rm mAP/input/ground-truth/*\n",
        "!rm mAP/input/images-optional/*\n",
        "!wget -P mAP https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/calculate_map_cartucho.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn22nGGqH5T6"
      },
      "source": [
        "Next, we'll copy the images and annotation data from the **test** folder to the appropriate folders inside the cloned repository. These will be used as the \"ground truth data\" that our model's detection results will be compared to.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5szFfVxwI3wT"
      },
      "outputs": [],
      "source": [
        "!cp images/test/* mAP/input/images-optional # Copy images and xml files\n",
        "!mv mAP/input/images-optional/*.xml mAP/input/ground-truth/  # Move xml files to the appropriate folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6aro817DGzx"
      },
      "source": [
        "The calculator tool expects annotation data in a format that's different from the Pascal VOC .xml file format we're using. Fortunately, it provides an easy script, `convert_gt_xml.py`, for converting to the expected .txt format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdjtOUDnK2AA"
      },
      "outputs": [],
      "source": [
        "!python mAP/scripts/extra/convert_gt_xml.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnIUacAlLP0B"
      },
      "source": [
        "Okay, we've set up the ground truth data, but now we need actual detection results from our model. The detection results will be compared to the ground truth data to calculate the model's accuracy in mAP.\n",
        "\n",
        "The inference function we defined in Step 7.1 can be used to generate detection data for all the images in the **test** folder. We'll use it the same as before, except this time we'll tell it to save detection results into the `detection-results` folder.\n",
        "\n",
        "Click Play to run the following code block!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szzHFAhsMNFF"
      },
      "outputs": [],
      "source": [
        "# Set up variables for running inference, this time to get detection results saved as .txt files\n",
        "PATH_TO_IMAGES='images/test'   # Path to test images folder\n",
        "PATH_TO_MODEL='custom_model_lite/detect.tflite'   # Path to .tflite model file\n",
        "PATH_TO_LABELS='labelmap.txt'   # Path to labelmap.txt file\n",
        "PATH_TO_RESULTS='mAP/input/detection-results' # Folder to save detection results in\n",
        "min_conf_threshold=0.1   # Confidence threshold\n",
        "\n",
        "# Use all the images in the test folder\n",
        "image_list = glob.glob(PATH_TO_IMAGES + '/*.jpg') + glob.glob(PATH_TO_IMAGES + '/*.JPG') + glob.glob(PATH_TO_IMAGES + '/*.png') + glob.glob(PATH_TO_IMAGES + '/*.bmp')\n",
        "images_to_test = min(500, len(image_list)) # If there are more than 500 images in the folder, just use 500\n",
        "\n",
        "# Tell function to just save results and not display images\n",
        "txt_only = True\n",
        "\n",
        "# Run inferencing function!\n",
        "print('Starting inference on %d images...' % images_to_test)\n",
        "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test, PATH_TO_RESULTS, txt_only)\n",
        "print('Finished inferencing!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_QRnTqNPX4z"
      },
      "source": [
        "Finally, let's calculate mAP! One popular style for reporting mAP is the COCO metric for mAP @ 0.50:0.95. Basically, this means that mAP is calculated at several IoU thresholds between 0.50 and 0.95, and then the result from each threshold is averaged to get a final mAP score. [Learn more here!](https://blog.roboflow.com/mean-average-precision/)\n",
        "\n",
        "I wrote a script to run the calculator tool at each IoU threshold, average the results, and report the final accuracy score. It reports mAP for each class and overall mAP. Click Play on the following two blocks to calculate mAP!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DkjpIBARTQ7"
      },
      "outputs": [],
      "source": [
        "!cd mAP/&& python calculate_map_cartucho.py --labels=../labelmap.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9HPoOBVKvxU"
      },
      "source": [
        "The score reported at the end is your model's overall mAP score. Ideally, it should be above 50% (0.50). If it isn't, you can increase your model's accuracy by adding more images to your dataset. See my [dataset video](https://www.youtube.com/watch?v=v0ssiOY6cfg) for tips on how to capture good training images and improve accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i40ve0SCLaE"
      },
      "source": [
        "# 9.&nbsp;Deploy TensorFlow Lite Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phT8vvzriqQp"
      },
      "source": [
        "Now that your custom model has been trained and converted to TFLite format, it's ready to be downloaded and deployed in an application! This section shows how to download the model and provides links to instructions for deploying it on the Raspberry Pi, your PC, or other edge devices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq3L2IoP4VHp"
      },
      "source": [
        "## 9.1. Download TFLite model\n",
        "\n",
        "Run the two following cells to copy the labelmap files into the model folder, compress it into a zip folder, and then download it. The zip folder contains the `detect.tflite` model and `labelmap.txt` labelmap files that are needed to run the model in your application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awZMQGVqMpVL"
      },
      "outputs": [],
      "source": [
        "# Move labelmap and pipeline config files into TFLite model folder and zip it up\n",
        "!cp labelmap.txt custom_model_lite\n",
        "!cp labelmap.pbtxt custom_model_lite\n",
        "!cp models/mymodel/pipeline_file.config custom_model_lite\n",
        "\n",
        "!zip -r custom_model_lite.zip custom_model_lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Kb3ZBsMq95l"
      },
      "source": [
        "The `custom_model_lite.zip` file containing the model will download into your Downloads folder. It's ready to be deployed on your device!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSJ2wgGCixy2"
      },
      "source": [
        "## 9.2. Deploy model\n",
        "TensorFlow Lite models can run on a wide variety of hardware, including PCs, embedded systems, and phones. This section provides instructions showing how to deploy your TFLite model on various devices.\n",
        "\n",
        "### 9.2.1. Deploy on Raspberry Pi\n",
        "TFLite models are great for running on the Raspberry Pi, because they require less processing power than regular TensorFlow vision models. The Pi can run TFLite models in near real-time.\n",
        "\n",
        "To run your new model on the Raspberry Pi, you'll have to install TensorFlow Lite and prepare a Python environment for your application. I provide step-by-step instructions on how to set up TFLite on the Pi in my video, [How To Run TensorFlow Lite on Raspberry Pi for Object Detection](https://youtu.be/aimSGOAUI8Y).\n",
        "\n",
        "[![Link to my YouTube video!](https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/doc/YouTube_video1.JPG)](https://www.youtube.com/watch?v=aimSGOAUI8Y)\n",
        "\n",
        "Once you've completed all the steps in the video, move the `custom_model_lite.zip` file downloaded from this Colab session over to your Raspberry Pi into the `~/tflite1` folder. Move into the folder and unzip it by issuing:\n",
        "\n",
        "```\n",
        "cd ~/tflite1\n",
        "unzip custom_model_lite.zip\n",
        "```\n",
        "\n",
        "Then, run the image, video, or webcam TFLite detection program with the `--modeldir=fine_tuned_model_lite` argument. For example, to run the webcam detection program, issue:\n",
        "\n",
        "```\n",
        "python TFLite_detection_webcam.py --modeldir=custom_model_lite\n",
        "```\n",
        "\n",
        "A window will appear showing a live feed from your webcam with boxes drawn around detected objects in each frame.\n",
        "\n",
        "### 9.2.2. Deploy on Windows, Linux, or macOS\n",
        "Follow the instructions linked below to quickly set up your Windows, Linux, or macOS computer to run TFLite models. It only takes a few minutes! Running a model on your PC is good for quickly testing your model with a webcam. However, keep in mind that the TFLite Runtime is optimized for lower-power processors, and it won't utilize the full capability of your PC's processor.\n",
        "\n",
        "Here are links to the deployment guides for Windows, Linux, and macOS:\n",
        "* [How to Run TensorFlow Lite Models on Windows](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/deploy_guides/Windows_TFLite_Guide.md)\n",
        "* *link to Linux guide to be added (but really it's the same as Raspberry Pi)*\n",
        "* [How to Run TensorFlow Lite Models on macOS](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/deploy_guides/MacOS_TFLite_Guide.md)\n",
        "\n",
        "### 9.2.3. Deploy on other Linux-based edge devices\n",
        "Instructions to be added! 🐧\n",
        "\n",
        "### 9.2.4. Deploy on Android\n",
        "Instructions to be added! 🤖\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoptFnAhCSrR"
      },
      "source": [
        "# 10.&nbsp;(Optional) Post-Training Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I54paUm8dUCr"
      },
      "source": [
        "Want to make your TFLite model run even faster? Quantize it! Quantizating a model converts its weights from 32-bit floating-point values to 8-bit integer values. This allows the quantized model to run faster and occupy less memory without too much reduction in accuracy.\n",
        "\n",
        "> Note: If you observe an obvious decrease in detection accuracy when quantizing your model with TF2, I recommend using TensorFlow 1 to quantize your model instead. TF1 supports quantization-aware training, which helps improve the accuracy of quantized models. The ssd-mobilenet-v2-quantized model from the TF1 Model Zoo has fast and accurate performance when trained with a custom dataset. Visit my [TFLite v1 Colab notebook](https://colab.research.google.com/github/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/Train_TFLite1_Object_Detection_Model.ipynb) for step-by-step instructions on how to train and quantize a model with TensorFlow 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTyqlXFTJ0Uv"
      },
      "source": [
        "## 10.1. Quantize model\n",
        "We'll use the \"TFLiteConverter\" module to perform [post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) on the model. To quantize the model, we need to provide a representative dataset, which is a set of images that represent what the model will see when deployed in the field. First, we'll create a list of images to include in the representative dataset (we'll just use the images in the `train` folder).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSNZtfj_k3NP"
      },
      "outputs": [],
      "source": [
        "# Get list of all images in train directory\n",
        "image_path = 'images/train'\n",
        "\n",
        "jpg_file_list = glob.glob(image_path + '/*.jpg')\n",
        "JPG_file_list = glob.glob(image_path + '/*.JPG')\n",
        "png_file_list = glob.glob(image_path + '/*.png')\n",
        "bmp_file_list = glob.glob(image_path + '/*.bmp')\n",
        "\n",
        "quant_image_list = jpg_file_list + JPG_file_list + png_file_list + bmp_file_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqbH1VlEgiuy"
      },
      "source": [
        "Next, we'll define a function to yield images from our representative dataset. Refer to [TensorFlow's sample quantization code](https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb#scrollTo=kRDabW_u1wnv) to get a better understanding of what this is doing!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORzx0XRErSLV"
      },
      "outputs": [],
      "source": [
        "# A generator that provides a representative dataset\n",
        "# Code modified from https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb\n",
        "\n",
        "# First, get input details for model so we know how to preprocess images\n",
        "interpreter = Interpreter(model_path=PATH_TO_MODEL) # PATH_TO_MODEL is defined in Step 7 above\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "height = input_details[0]['shape'][1]\n",
        "width = input_details[0]['shape'][2]\n",
        "\n",
        "import random\n",
        "\n",
        "def representative_data_gen():\n",
        "  dataset_list = quant_image_list\n",
        "  quant_num = 300\n",
        "  for i in range(quant_num):\n",
        "    pick_me = random.choice(dataset_list)\n",
        "    image = tf.io.read_file(pick_me)\n",
        "\n",
        "    if pick_me.endswith('.jpg') or pick_me.endswith('.JPG'):\n",
        "      image = tf.io.decode_jpeg(image, channels=3)\n",
        "    elif pick_me.endswith('.png'):\n",
        "      image = tf.io.decode_png(image, channels=3)\n",
        "    elif pick_me.endswith('.bmp'):\n",
        "      image = tf.io.decode_bmp(image, channels=3)\n",
        "\n",
        "    image = tf.image.resize(image, [width, height])  # TO DO: Replace 300s with an automatic way of reading network input size\n",
        "    image = tf.cast(image / 255., tf.float32)\n",
        "    image = tf.expand_dims(image, 0)\n",
        "    yield [image]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqtu98mzebEj"
      },
      "source": [
        "Finally, we'll initialize the TFLiteConverter module, point it at the TFLite graph we generated in Step 6, and provide it with the representative dataset generator function we created in the previous code block. We'll configure the converter to quantize the model's weight values to INT8 format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox0bGDWds_Ce"
      },
      "outputs": [],
      "source": [
        "# Initialize converter module\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('custom_model_lite/saved_model')\n",
        "\n",
        "# This enables quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# This sets the representative dataset for quantization\n",
        "converter.representative_dataset = representative_data_gen\n",
        "# This ensures that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\n",
        "converter.target_spec.supported_types = [tf.int8]\n",
        "# These set the input tensors to uint8 and output tensors to float32\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.float32\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('custom_model_lite/detect_quant.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYVVlv5QUUZF"
      },
      "source": [
        "## 10.2. Test quantized model\n",
        "The model has been quantized and exported as `detect_quant.tflite`. Let's test it out! We'll re-use the function from Section 7 for running the model on test images and display the results, except this time we'll point it at the quantized model.\n",
        "\n",
        "Click Play on the code block below to test the `detect_quant.tflite` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OoirJuOtdOG"
      },
      "outputs": [],
      "source": [
        "# Set up parameters for inferencing function (using detect_quant.tflite instead of detect.tflite)\n",
        "PATH_TO_IMAGES='images/test'   #Path to test images folder\n",
        "PATH_TO_MODEL='custom_model_lite/detect_quant.tflite'   #Path to .tflite model file\n",
        "PATH_TO_LABELS='labelmap.txt'   #Path to labelmap.txt file\n",
        "min_conf_threshold=0.5   #Confidence threshold (try changing this to 0.01 if you don't see any detection results)\n",
        "images_to_test = 10   #Number of images to run detection on\n",
        "\n",
        "# Run inferencing function!\n",
        "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKo7ZtfOyoxG"
      },
      "source": [
        "If your quantized model isn't performing very well, try using my TensorFlow Lite 1 notebook *(link to be added)* to train a SSD-MobileNet model with your dataset. In my experience, the `ssd-mobilenet-v2-quantized` model from the [TF1 Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md) has the best quantized performance out of any other TensorFlow Lite model.\n",
        "\n",
        "TFLite models created with TensorFlow 1 are still compatible with the TensorFlow Lite 2 runtime, so your TFLite 1 model will still work with my [TensorFlow setup guide for the Raspberry Pi](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/Raspberry_Pi_Guide.md)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWdVxs6LUjbR"
      },
      "source": [
        "## 10.3 Calculate quantized model mAP\n",
        "\n",
        "Let's calculate the quantize model's mAP using the calculator tool we set up in Step 7.2. We just need to perform inference with our quantized model (`detect_quant.tflite`) to get a new set of detection results.\n",
        "\n",
        "Run the following block to run inference on the test images and save the detection results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMaumV-11Et0"
      },
      "outputs": [],
      "source": [
        "# Need to remove existing detection results first\n",
        "!rm mAP/input/detection-results/*\n",
        "\n",
        "# Set up variables for running inference, this time to get detection results saved as .txt files\n",
        "PATH_TO_IMAGES='images/test'   # Path to test images folder\n",
        "PATH_TO_MODEL='custom_model_lite/detect_quant.tflite'   # Path to quantized .tflite model file\n",
        "PATH_TO_LABELS='labelmap.txt'   # Path to labelmap.txt file\n",
        "PATH_TO_RESULTS='mAP/input/detection-results' # Folder to save detection results in\n",
        "min_conf_threshold=0.1   # Confidence threshold\n",
        "\n",
        "# Use all the images in the test folder\n",
        "image_list = glob.glob(PATH_TO_IMAGES + '/*.jpg') + glob.glob(PATH_TO_IMAGES + '/*.JPG') + glob.glob(PATH_TO_IMAGES + '/*.png') + glob.glob(PATH_TO_IMAGES + '/*.bmp')\n",
        "images_to_test = min(500, len(image_list)) # If there are more than 500 images in the folder, just use 500\n",
        "\n",
        "# Tell function to just save results and not display images\n",
        "txt_only = True\n",
        "\n",
        "# Run inferencing function!\n",
        "print('Starting inference on %d images...' % images_to_test)\n",
        "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test, PATH_TO_RESULTS, txt_only)\n",
        "print('Finished inferencing!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgcmdLQf1Et1"
      },
      "source": [
        "Now we can run the mAP calculation script to determine our quantized model's mAP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIRNp0Af1Et1"
      },
      "outputs": [],
      "source": [
        "!cd mAP && python calculate_map_cartucho.py --labels=../labelmap.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFsuasvxFHo8"
      },
      "source": [
        "## 10.4. Compile model for Edge TPU\n",
        "\n",
        "Now that the model has been converted to TFLite and quantized, we can compile it to run on Edge TPU devices like the [Coral USB Accelerator](https://coral.ai/products/accelerator/) or the [Coral Dev Board](https://coral.ai/products/dev-board/). This allows the model to run much faster! For information on how to set up the USB Accelerator, my [TensorFlow Lite repository on GitHub](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/deploy_guides/Raspberry_Pi_Guide.md#section-2---run-edge-tpu-object-detection-models-on-the-raspberry-pi-using-the-coral-usb-accelerator).\n",
        "\n",
        "First, install the Edge TPU Compiler package inside this Colab instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUd_SNC0JSq0"
      },
      "outputs": [],
      "source": [
        "! curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "! echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "! sudo apt-get update\n",
        "! sudo apt-get install edgetpu-compiler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usfmdtSiJuuC"
      },
      "source": [
        "Next, compile the quantize TFLite model. (If your model has a different filename than \"detect_quant.tflite\", use that instead.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mULCY0nb0ahH"
      },
      "outputs": [],
      "source": [
        "%cd /content/custom_model_lite\n",
        "!edgetpu_compiler detect_quant.tflite\n",
        "!mv detect_quant_edgetpu.tflite edgetpu.tflite\n",
        "!rm detect_quant_edgetpu.log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqGy2FgzKomN"
      },
      "source": [
        "The compiled model will be output in the `custom_model_lite` folder as \"detect__quant_edgetpu.tflite\". It gets renamed to \"edgetpu.tflite\" to be consistent with my code. Zip the `custom_model_lite` folder and download it by running the two code blocks below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nCdUouYJjQM"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!zip -r custom_model_lite.zip custom_model_lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmjqvKuuK8ZR"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('custom_model_lite.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptwpBBEWLfuJ"
      },
      "source": [
        "Now you're all set to use the Coral model! For instructions on how to run an object detection model on the Raspberry Pi using the Coral USB Acclerator, please see my video, [\"How to Use the Coral USB Accelerator with the Raspberry Pi\"](https://www.youtube.com/watch?v=qJMwNHQNOVU)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VI_Gh5dCd7w"
      },
      "source": [
        "# Appendix: Common Errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEbd9cO7I_o3"
      },
      "source": [
        "Here are solutions to common errors that can occur while stepping through this notebook.\n",
        "\n",
        "**1. Training suddenly stops with ^C output**\n",
        "\n",
        "If your training randomly stops without any error messages except a `^C`, that means the virtual machine has run out of memory. To resolve the issue, try reducing the `batch_size` variable in Step 4 to a lower value like `batch_size = 4`. The value must be a power of 2. (e.g. 2, 4, 8 ...)\n",
        "\n",
        "Source: https://stackoverflow.com/questions/75901898/why-my-model-training-automatically-stopped-during-training"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4VAvZo8qE4u5",
        "sxb8_h-QFErO",
        "eydREUsMGUUR",
        "eGEUZYAMEZ6f",
        "-19zML6oEO7l",
        "kPg8oMnQDYKl",
        "RDQrtQhvC3oG",
        "5i40ve0SCLaE",
        "WoptFnAhCSrR",
        "5VI_Gh5dCd7w"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
